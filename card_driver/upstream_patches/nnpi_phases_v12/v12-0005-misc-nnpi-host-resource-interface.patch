From 2443f76f72ba1a65a457332489713056d6ebc709 Mon Sep 17 00:00:00 2001
From: Guy Zadicario <guy.zadicario@intel.com>
Date: Sun, 15 Nov 2020 08:03:56 +0200
Subject: [PATCH v12 05/29] misc: nnpi: host resource interface

Provide interface for creating a "host resource" - a memory object which
can be mapped to a dma address space of one or more NNP-I devices.
These host resource objects manage memory blocks from which big chunks
of data, such as inference tensors, the device's embedded OS and telemtry
data, are DMA'ed to and from the NNP-I device.

Signed-off-by: Guy Zadicario <guy.zadicario@intel.com>
Reviewed-by: Vaibhav Agarwal <vaibhav.agarwal@intel.com>
---
 Documentation/ABI/testing/sysfs-driver-intel_nnpi |   5 +
 drivers/misc/intel-nnpi/Makefile                  |   3 +-
 drivers/misc/intel-nnpi/hostres.c                 | 678 ++++++++++++++++++++++
 drivers/misc/intel-nnpi/hostres.h                 | 196 +++++++
 4 files changed, 881 insertions(+), 1 deletion(-)
 create mode 100644 Documentation/ABI/testing/sysfs-driver-intel_nnpi
 create mode 100644 drivers/misc/intel-nnpi/hostres.c
 create mode 100644 drivers/misc/intel-nnpi/hostres.h

diff --git a/Documentation/ABI/testing/sysfs-driver-intel_nnpi b/Documentation/ABI/testing/sysfs-driver-intel_nnpi
new file mode 100644
index 0000000..ce8b68d
--- /dev/null
+++ b/Documentation/ABI/testing/sysfs-driver-intel_nnpi
@@ -0,0 +1,5 @@
+What:           /sys/class/nnpi_host/nnpi_host/total_hostres_size
+Date:           Sep 2020
+Kernelversion:  5.11
+Contact:        guy.zadicario@intel.com
+Description:    Total size in bytes of all allocated NNP-I host resources.
diff --git a/drivers/misc/intel-nnpi/Makefile b/drivers/misc/intel-nnpi/Makefile
index a0685d3..b6f962e 100644
--- a/drivers/misc/intel-nnpi/Makefile
+++ b/drivers/misc/intel-nnpi/Makefile
@@ -5,6 +5,7 @@
 
 obj-m	:= intel_nnpidrv.o
 
-intel_nnpidrv-y := nnpdrv_main.o nnp_hw_pcie.o device.o msg_scheduler.o
+intel_nnpidrv-y := nnpdrv_main.o nnp_hw_pcie.o device.o msg_scheduler.o \
+		   hostres.o
 
 ccflags-y += -I$(srctree)/$(src)/ipc_include
diff --git a/drivers/misc/intel-nnpi/hostres.c b/drivers/misc/intel-nnpi/hostres.c
new file mode 100644
index 0000000..4351506
--- /dev/null
+++ b/drivers/misc/intel-nnpi/hostres.c
@@ -0,0 +1,678 @@
+// SPDX-License-Identifier: GPL-2.0-only
+
+/* Copyright (C) 2019-2020 Intel Corporation */
+
+#define pr_fmt(fmt)   KBUILD_MODNAME ": " fmt
+
+#include "hostres.h"
+#include <linux/atomic.h>
+#include <linux/bitfield.h>
+#include <linux/err.h>
+#include <linux/jiffies.h>
+#include <linux/module.h>
+#include <linux/pagemap.h>
+#include <linux/printk.h>
+#include <linux/sched.h>
+#include <linux/sched/mm.h>
+#include <linux/slab.h>
+#include <linux/vmalloc.h>
+#include <linux/wait.h>
+#include "ipc_protocol.h"
+
+/**
+ * struct dev_mapping - mapping information of host resource to one device
+ * @ref: kref for that mapping object
+ * @res: pointer to the host resource
+ * @dev: the device the resource is mapped to
+ * @dir: dma direction mask possible for this mapping
+ * @sgt: scatter table of host resource pages in memory
+ * @dma_chain_sgt: sg_table of dma_chain blocks (see description below).
+ * @dma_chain_order: order used to allocate scatterlist of @dma_chain_sgt.
+ * @node: list head to attach this object to a list of mappings
+ * @dma_att: dma-buf attachment in case the resource created from dma-buf,
+ *           if it is not a dma-buf resource, this will remain NULL.
+ *
+ * This structure holds mapping information of one host resource to one
+ * NNP-I device. @sgt is the sg_table describes the dma addresses of the
+ * resource chunks.
+ *
+ * Explanation of @dma_chain_sgt:
+ * When mapping a host memory resource for NNP-I device access, we need to send
+ * the dma page table of the resource to the device. The device uses this page
+ * table when programming its DMA engine to read/write the host resource.
+ *
+ * The format of that page table is a chain of continuous dma buffers, each
+ * starts with a 24 bytes header (struct dma_chain_header, defined in
+ * ipc_protocol.h) followed by 8 bytes entries, each describe a continuous
+ * block of the resource (struct dma_chain_entry, defined in ipc_procol.h).
+ *
+ * The header of the chain has a pointer to the next buffer in the chain for
+ * the case where multiple dma blocks are required to describe the
+ * entire resource. The address of the first block in the chain is sent to
+ * the device, which then fetches the entire chain when the resource is
+ * mapped.
+ * @dma_chain_sgt is an sg_table of memory mapped to the device and initialized
+ * with the resource page table in the above described format.
+ */
+struct dev_mapping {
+	struct kref                 ref;
+	struct host_resource        *res;
+	struct device               *dev;
+	enum dma_data_direction     dir;
+	struct sg_table             *sgt;
+	struct sg_table             dma_chain_sgt;
+	unsigned int                dma_chain_order;
+	struct list_head            node;
+	struct dma_buf_attachment   *dma_att;
+};
+
+/**
+ * struct host_resource - structure for host memory resource object
+ * @ref: kref for that mapping object
+ * @size: size of the memory resource, in bytes
+ * @devices: list of devices this resource is mapped to (list of dev_mapping)
+ * @lock: protects fields modifications in this structure.
+ * @dir: specify if the resource can be copied to/from a device, or both.
+ * @pinned_mm: mm object used to pin the user allocated resource memory. NULL
+ *             if the resource was not allocated by user-space.
+ * @vptr: virtual pointer to the resource memory if allocated by
+ *        nnp_hostres_alloc(). NULL otherwise.
+ * @start_offset: relevant only when @pinned_mm != NULL, 0 otherwise.
+ *                holds the offset within the first pinned page where resource
+ *                memory starts.
+ * @pages: array of resource memory pages.
+ * @n_pages: size of pages array.
+ */
+struct host_resource {
+	struct kref       ref;
+	size_t            size;
+	struct list_head  devices;
+	spinlock_t        lock; /* protects fields in this struct */
+	enum dma_data_direction dir;
+
+	struct mm_struct  *pinned_mm;
+	void              *vptr;
+	unsigned int      start_offset;
+
+	struct page       **pages;
+	unsigned int      n_pages;
+};
+
+/*
+ * Since host resources are pinned for their entire lifetime, it
+ * is useful to monitor the total size of NNP-I host resources
+ * allocated in the system.
+ */
+static atomic64_t total_hostres_size;
+
+/* Destroys host resource, when all references to it are released */
+static void release_hostres(struct kref *kref)
+{
+	struct host_resource *r = container_of(kref, struct host_resource, ref);
+
+	if (!r->pinned_mm) {
+		vfree(r->vptr);
+	} else {
+		unpin_user_pages(r->pages, r->n_pages);
+		account_locked_vm(r->pinned_mm, r->n_pages, false);
+		mmdrop(r->pinned_mm);
+	}
+
+	kvfree(r->pages);
+	atomic64_sub(r->size, &total_hostres_size);
+	kfree(r);
+}
+
+void nnp_hostres_get(struct host_resource *res)
+{
+	kref_get(&res->ref);
+};
+
+void nnp_hostres_put(struct host_resource *res)
+{
+	kref_put(&res->ref, release_hostres);
+}
+
+/* Really destroys mapping to device, when refcount is zero */
+static void release_mapping(struct kref *kref)
+{
+	struct dev_mapping *m = container_of(kref, struct dev_mapping, ref);
+
+	dma_unmap_sgtable(m->dev, &m->dma_chain_sgt, DMA_TO_DEVICE, 0);
+	sgl_free_order(m->dma_chain_sgt.sgl, m->dma_chain_order);
+
+	dma_unmap_sg(m->dev, m->sgt->sgl,
+		     m->sgt->orig_nents, m->res->dir);
+	sg_free_table(m->sgt);
+	kfree(m->sgt);
+
+	spin_lock(&m->res->lock);
+	list_del(&m->node);
+	spin_unlock(&m->res->lock);
+
+	nnp_hostres_put(m->res);
+
+	kfree(m);
+}
+
+static struct host_resource *alloc_hostres(size_t size,
+					   enum dma_data_direction dir)
+{
+	struct host_resource *r;
+
+	r = kzalloc(sizeof(*r), GFP_KERNEL);
+	if (!r)
+		return r;
+
+	kref_init(&r->ref);
+	spin_lock_init(&r->lock);
+	r->dir = dir;
+	r->size = size;
+	INIT_LIST_HEAD(&r->devices);
+
+	return r;
+}
+
+struct host_resource *nnp_hostres_alloc(size_t size,
+					enum dma_data_direction dir)
+{
+	int err = -ENOMEM;
+	struct host_resource *r;
+	unsigned int i;
+	char *p;
+
+	if (size == 0 || dir == DMA_NONE)
+		return ERR_PTR(-EINVAL);
+
+	r = alloc_hostres(size, dir);
+	if (!r)
+		return ERR_PTR(err);
+
+	r->n_pages = DIV_ROUND_UP(size, PAGE_SIZE);
+	r->vptr = vzalloc(r->n_pages * PAGE_SIZE);
+	if (!r->vptr)
+		goto free_res;
+
+	r->pages = kvmalloc_array(r->n_pages, sizeof(struct page *),
+				  GFP_KERNEL);
+	if (!r->pages)
+		goto free_vptr;
+
+	for (i = 0, p = r->vptr; i < r->n_pages; ++i, p += PAGE_SIZE) {
+		r->pages[i] = vmalloc_to_page(p);
+		if (!r->pages[i])
+			goto free_pages;
+	}
+
+	atomic64_add(size, &total_hostres_size);
+
+	return r;
+
+free_pages:
+	kvfree(r->pages);
+free_vptr:
+	vfree(r->vptr);
+free_res:
+	kfree(r);
+	return ERR_PTR(err);
+}
+
+struct host_resource *nnp_hostres_from_usermem(void __user *user_ptr,
+					       size_t size,
+					       enum dma_data_direction dir)
+{
+	int err;
+	struct host_resource *r;
+	unsigned int pinned_pages = 0;
+	uintptr_t user_addr = (uintptr_t)user_ptr;
+	int gup_flags;
+
+	if (size == 0 || dir == DMA_NONE)
+		return ERR_PTR(-EINVAL);
+
+	/* restrict for 4 byte alignment */
+	if ((user_addr & 0x3) != 0)
+		return ERR_PTR(-EINVAL);
+
+	if (!access_ok(user_ptr, size))
+		return ERR_PTR(-EFAULT);
+
+	r = alloc_hostres(size, dir);
+	if (!r)
+		return ERR_PTR(-ENOMEM);
+
+	r->start_offset = offset_in_page(user_addr);
+	user_addr &= PAGE_MASK;
+
+	/*
+	 * In this place actual pages are allocated of PAGE_SIZE and not
+	 * NNP_PAGE_SIZE, This list will be used for sg_alloc_table
+	 */
+	r->n_pages = DIV_ROUND_UP(size + r->start_offset, PAGE_SIZE);
+	r->pages = kvmalloc_array(r->n_pages, sizeof(struct page *), GFP_KERNEL);
+	if (!r->pages) {
+		err = -ENOMEM;
+		goto free_res;
+	}
+
+	err = account_locked_vm(current->mm, r->n_pages, true);
+	if (err)
+		goto free_pages;
+
+	gup_flags = (dir == DMA_TO_DEVICE ||
+		     dir == DMA_BIDIRECTIONAL) ? FOLL_WRITE : 0;
+
+	/*
+	 * The host resource is being re-used for multiple DMA
+	 * transfers for streaming data into the device.
+	 * In most situations will live long term.
+	 */
+	gup_flags |= FOLL_LONGTERM;
+
+	do {
+		int n = pin_user_pages(user_addr + pinned_pages * PAGE_SIZE,
+				       r->n_pages - pinned_pages, gup_flags,
+				       &r->pages[pinned_pages], NULL);
+		if (n < 0) {
+			err = -ENOMEM;
+			goto unaccount;
+		}
+
+		pinned_pages += n;
+	} while (pinned_pages < r->n_pages);
+
+	r->pinned_mm = current->mm;
+	mmgrab(r->pinned_mm);
+
+	atomic64_add(size, &total_hostres_size);
+
+	return r;
+
+unaccount:
+	account_locked_vm(current->mm, r->n_pages, false);
+free_pages:
+	unpin_user_pages(r->pages, pinned_pages);
+	kvfree(r->pages);
+free_res:
+	kfree(r);
+	return ERR_PTR(err);
+}
+
+/* Finds mapping by device and increase its refcount. NULL if not found*/
+static struct dev_mapping *get_mapping_for_dev(struct host_resource *res,
+					       struct device *dev)
+{
+	struct dev_mapping *m;
+
+	spin_lock(&res->lock);
+
+	list_for_each_entry(m, &res->devices, node) {
+		if (m->dev == dev) {
+			kref_get(&m->ref);
+			goto out;
+		}
+	}
+
+	m = NULL;
+out:
+	spin_unlock(&res->lock);
+	return m;
+}
+
+/* Finds mapping by device and decrease its refcount */
+static int put_mapping_for_dev(struct host_resource *res,
+			       struct device *dev)
+{
+	struct dev_mapping *m;
+
+	m = get_mapping_for_dev(res, dev);
+	if (!m)
+		return -ENOENT;
+
+	/* put once for the get done in get_mapping_for_dev */
+	kref_put(&m->ref, release_mapping);
+
+	/* put for the caller request */
+	kref_put(&m->ref, release_mapping);
+
+	return 0;
+}
+
+static bool entry_valid(struct scatterlist *sgl, u64 ipc_entry)
+{
+	unsigned long long pfn;
+	unsigned long n_pages;
+
+	pfn = FIELD_GET(DMA_CHAIN_ENTRY_PFN_MASK, ipc_entry);
+	if (NNP_IPC_DMA_PFN_TO_ADDR(pfn) != sg_dma_address(sgl))
+		return false;
+
+	n_pages = FIELD_GET(DMA_CHAIN_ENTRY_NPAGES_MASK, ipc_entry);
+	if (n_pages != DIV_ROUND_UP(sg_dma_len(sgl), NNP_PAGE_SIZE))
+		return false;
+
+	return true;
+}
+
+/**
+ * build_ipc_dma_chain_array() - builds page list of the resource for ipc usage
+ * @m: pointer to device mapping info struct
+ * @use_one_entry: if true will generate all page table in one continuous
+ *                 dma chunk. otherwise a chain of blocks will be used
+ *                 each of one page size.
+ * @start_offset: offset in first mapped page where resource memory starts,
+ *
+ * This function allocates scatterlist, map it to device and populate it with
+ * page table of the device mapped resource in format suitable to be used
+ * in the ipc protocol for sending the resource page table to the device.
+ * The format of the page table is described in the documenation of struct
+ * dev_mapping.
+ *
+ * Return: 0 on success, error value otherwise
+ */
+static int build_ipc_dma_chain_array(struct dev_mapping *m, bool use_one_entry,
+				     unsigned int start_offset)
+{
+	unsigned int i, k = 0;
+	int err = -ENOMEM;
+	u64 *p = NULL;
+	struct dma_chain_header *h;
+	struct scatterlist *sg, *map_sg;
+	struct scatterlist *chain_sg;
+	unsigned int chain_size;
+	unsigned int chain_order;
+	unsigned int chain_nents;
+	unsigned int nents_per_entry;
+	dma_addr_t addr;
+	unsigned long long pfn, size;
+	unsigned long n_pages;
+	int rc;
+	u64 e;
+
+	if (use_one_entry) {
+		/*
+		 * Allocate enough pages in one chunk that will fit
+		 * the header and dma_chain_entry for all the sg_table
+		 * entries.
+		 */
+		nents_per_entry = m->sgt->nents;
+		chain_size = sizeof(struct dma_chain_header) +
+			     m->sgt->nents * DMA_CHAIN_ENTRY_SIZE;
+		chain_order = get_order(chain_size);
+	} else {
+		/*
+		 * calc number of one page dma buffers needed to hold the
+		 * entire page table.
+		 * NENTS_PER_PAGE is how much dma chain entries fits
+		 * in a single page following the chain header, must be at
+		 * positive.
+		 */
+		BUILD_BUG_ON(NENTS_PER_PAGE < 1);
+		nents_per_entry = NENTS_PER_PAGE;
+		chain_size = DIV_ROUND_UP(m->sgt->nents, NENTS_PER_PAGE) *
+			     NNP_PAGE_SIZE;
+		chain_order = 0;
+	}
+
+	chain_sg = sgl_alloc_order(chain_size, chain_order,
+				   false, GFP_KERNEL, &chain_nents);
+	if (!chain_sg)
+		return -ENOMEM;
+
+	m->dma_chain_sgt.sgl = chain_sg;
+	m->dma_chain_sgt.nents = chain_nents;
+	m->dma_chain_sgt.orig_nents = chain_nents;
+	m->dma_chain_order = chain_order;
+	rc = dma_map_sgtable(m->dev, &m->dma_chain_sgt, DMA_TO_DEVICE, 0);
+	if (rc)
+		goto free_chain_sg;
+
+	/*
+	 * initialize chain entry blocks
+	 */
+	map_sg = m->sgt->sgl;
+	for_each_sg(chain_sg, sg, chain_nents, i) {
+		/*
+		 * Check that the allocated dma address fits in ipc protocol.
+		 * In the protocol, dma addresses are sent as 4K page numbers
+		 * and must fit in 45 bits.
+		 * Meaning, if the dma address is larger than 57 bits it will
+		 * not fit.
+		 */
+		addr = sg_dma_address(sg);
+		if (NNP_IPC_DMA_PFN_TO_ADDR(NNP_IPC_DMA_ADDR_TO_PFN(addr)) !=
+				addr)
+			goto unmap_chain_sg;
+
+		/* h: points to the header of current block */
+		h = sg_virt(sg);
+
+		/* p: points to current chunk entry in block */
+		p = (u64 *)(h + 1);
+		size = 0;
+		for (k = 0; k < nents_per_entry && map_sg; ++k) {
+			/*
+			 * build entry with dma address as page number and
+			 * size in pages
+			 */
+			pfn = NNP_IPC_DMA_ADDR_TO_PFN(sg_dma_address(map_sg));
+			n_pages = DIV_ROUND_UP(sg_dma_len(map_sg), NNP_PAGE_SIZE);
+
+			e = FIELD_PREP(DMA_CHAIN_ENTRY_PFN_MASK, pfn);
+			e |= FIELD_PREP(DMA_CHAIN_ENTRY_NPAGES_MASK, n_pages);
+
+			/*
+			 * Check that packed entry matches the dma chunk.
+			 * (Will fail if either pfn or n_pages fields overflows)
+			 */
+			if (!entry_valid(map_sg, e))
+				goto unmap_chain_sg;
+
+			/* Fill entry value (should be 64-bit little-endian) */
+			p[k] = cpu_to_le64(e);
+
+			size += sg_dma_len(map_sg);
+
+			map_sg = sg_next(map_sg);
+		}
+
+		/* initialize block header and link to next block */
+		h->total_nents = cpu_to_le32(m->sgt->nents);
+		h->start_offset = (i == 0 ? cpu_to_le32(start_offset) : 0);
+		h->size = cpu_to_le64(size);
+		if (sg_is_last(sg))
+			h->dma_next = 0;
+		else
+			h->dma_next = cpu_to_le64(sg_dma_address(sg_next(sg)));
+	}
+
+	return 0;
+
+unmap_chain_sg:
+	dma_unmap_sgtable(m->dev, &m->dma_chain_sgt, DMA_TO_DEVICE, 0);
+free_chain_sg:
+	sgl_free_order(chain_sg, chain_order);
+	memset(&m->dma_chain_sgt, 0, sizeof(m->dma_chain_sgt));
+	return err;
+}
+
+int nnp_hostres_map_device(struct host_resource *res,
+			   struct nnp_device *nnpdev, bool use_one_entry,
+			   dma_addr_t *page_list, unsigned int *total_chunks)
+{
+	int ret;
+	struct dev_mapping *m;
+	struct scatterlist *sge;
+
+	if (!res || !nnpdev || !page_list)
+		return -EINVAL;
+
+	/* Check if already mapped for the device */
+	m = get_mapping_for_dev(res, nnpdev->hw_dev->dev);
+	if (m) {
+		*page_list = sg_dma_address(m->dma_chain_sgt.sgl);
+		return 0;
+	}
+
+	nnp_hostres_get(res);
+
+	m = kmalloc(sizeof(*m), GFP_KERNEL);
+	if (!m) {
+		ret = -ENOMEM;
+		goto put_resource;
+	}
+
+	kref_init(&m->ref);
+
+	m->dev = nnpdev->hw_dev->dev;
+	m->res = res;
+
+	m->sgt = kmalloc(sizeof(*m->sgt), GFP_KERNEL);
+	if (!m->sgt) {
+		ret = -ENOMEM;
+		goto free_mapping;
+	}
+
+	sge = __sg_alloc_table_from_pages(m->sgt, res->pages, res->n_pages, 0,
+					  res->size + res->start_offset,
+					  NNP_MAX_CHUNK_SIZE, NULL, 0,
+					  GFP_KERNEL);
+	if (IS_ERR(sge)) {
+		ret = PTR_ERR(sge);
+		goto free_sgt_struct;
+	}
+
+	ret = dma_map_sg(m->dev, m->sgt->sgl,
+			 m->sgt->orig_nents, res->dir);
+	if (ret < 0)
+		goto free_sgt;
+
+	m->sgt->nents = ret;
+
+	ret = build_ipc_dma_chain_array(m, use_one_entry, res->start_offset);
+	if (ret < 0)
+		goto unmap;
+
+	spin_lock(&res->lock);
+	list_add(&m->node, &res->devices);
+	spin_unlock(&res->lock);
+
+	*page_list = sg_dma_address(m->dma_chain_sgt.sgl);
+	if (total_chunks)
+		*total_chunks = m->sgt->nents;
+
+	return 0;
+
+unmap:
+	dma_unmap_sg(m->dev, m->sgt->sgl,
+		     m->sgt->orig_nents, res->dir);
+free_sgt:
+	sg_free_table(m->sgt);
+free_sgt_struct:
+	kfree(m->sgt);
+free_mapping:
+	kfree(m);
+put_resource:
+	nnp_hostres_put(res);
+	return ret;
+}
+
+int nnp_hostres_unmap_device(struct host_resource *res,
+			     struct nnp_device *nnpdev)
+{
+	if (!res)
+		return -EINVAL;
+
+	return put_mapping_for_dev(res, nnpdev->hw_dev->dev);
+}
+
+int nnp_hostres_user_lock(struct host_resource *res)
+{
+	struct dev_mapping *m;
+
+	long ret = 0;
+
+	if (!res)
+		return -EINVAL;
+
+	spin_lock(&res->lock);
+	list_for_each_entry(m, &res->devices, node)
+		dma_sync_sg_for_cpu(m->dev, m->sgt->sgl,
+				    m->sgt->orig_nents, res->dir);
+	spin_unlock(&res->lock);
+
+	return ret;
+}
+
+int nnp_hostres_user_unlock(struct host_resource *res)
+{
+	struct dev_mapping *m;
+
+	if (!res)
+		return -EINVAL;
+
+	spin_lock(&res->lock);
+	list_for_each_entry(m, &res->devices, node)
+		dma_sync_sg_for_device(m->dev, m->sgt->sgl,
+				       m->sgt->orig_nents, res->dir);
+	spin_unlock(&res->lock);
+
+	return 0;
+}
+
+bool nnp_hostres_is_input(struct host_resource *res)
+{
+	return (res->dir == DMA_TO_DEVICE || res->dir == DMA_BIDIRECTIONAL);
+}
+
+bool nnp_hostres_is_output(struct host_resource *res)
+{
+	return (res->dir == DMA_FROM_DEVICE || res->dir == DMA_BIDIRECTIONAL);
+}
+
+size_t nnp_hostres_get_size(struct host_resource *res)
+{
+	return res->size;
+}
+
+bool nnp_hostres_is_usermem(struct host_resource *res)
+{
+	return res->pinned_mm;
+}
+
+void *nnp_hostres_vptr(struct host_resource *res)
+{
+	return res->vptr;
+}
+
+static ssize_t total_hostres_size_show(struct device *dev,
+				       struct device_attribute *attr, char *buf)
+{
+	return sprintf(buf, "%llu\n", (u64)atomic64_read(&total_hostres_size));
+}
+static DEVICE_ATTR_RO(total_hostres_size);
+
+static struct attribute *nnp_host_attrs[] = {
+	&dev_attr_total_hostres_size.attr,
+	NULL,
+};
+
+static struct attribute_group nnp_host_attrs_grp = {
+		.attrs = nnp_host_attrs,
+};
+
+int nnp_hostres_init_sysfs(struct device *dev)
+{
+	int ret;
+
+	atomic64_set(&total_hostres_size, 0);
+
+	ret = devm_device_add_group(dev, &nnp_host_attrs_grp);
+
+	return ret;
+}
+
+void nnp_hostres_fini_sysfs(struct device *dev)
+{
+	devm_device_remove_group(dev, &nnp_host_attrs_grp);
+}
diff --git a/drivers/misc/intel-nnpi/hostres.h b/drivers/misc/intel-nnpi/hostres.h
new file mode 100644
index 0000000..3bd0de2
--- /dev/null
+++ b/drivers/misc/intel-nnpi/hostres.h
@@ -0,0 +1,196 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+
+/* Copyright (C) 2019-2020 Intel Corporation */
+
+#ifndef _NNPDRV_HOSTRES_H
+#define _NNPDRV_HOSTRES_H
+
+#include <linux/dma-mapping.h>
+#include "device.h"
+
+struct host_resource;
+
+/**
+ * nnp_hostres_alloc() - allocate memory and create host resource
+ * @size: Size of the host resource to be created
+ * @dir:  Resource direction (read or write or both)
+ *
+ * This function allocates memory pages and provides host resource handle.
+ * The memory is zero filled and mapped to kernel virtual address.
+ * The resource can be Input(read by device), Output(write by device) and both.
+ *
+ * The return handle can be used as argument to one of the other functions
+ * in this file for:
+ *    - mapping/unmapping the resource for NNP-I device.
+ *    - pointer to the allocated memory can be retrieved by nnp_hostres_vptr()
+ *
+ * The handle should be released when no longer needed by a call to
+ * nnp_hostres_put.
+ *
+ * Return: pointer to created resource or error value
+ */
+struct host_resource *nnp_hostres_alloc(size_t size,
+					enum dma_data_direction dir);
+
+/**
+ * nnp_hostres_from_usermem() - Creates host resource from user-space memory
+ *
+ * @user_ptr: user virtual memory to pin
+ * @size: size of user buffer to pin
+ * @dir: Resource direction (read or write or both)
+ *
+ * This function pins the provided user memory and create a host resource
+ * handle managing this memory.
+ * The provided handle can be used the same as the handle created by
+ * nnp_hostres_alloc.
+ * The resource can be Input(read by device), Output(write by device) and both.
+ *
+ * The handle should be released when no longer needed by a call to
+ * nnp_hostres_put.
+ *
+ * Return: pointer to created resource or error value
+ */
+struct host_resource *nnp_hostres_from_usermem(void __user *user_ptr,
+					       size_t size,
+					       enum dma_data_direction dir);
+
+/**
+ * nnp_hostres_map_device() - Maps the host resource to NNP-I device
+ *
+ * @res: handle to host resource
+ * @nnpdev: handle to nnp device struct
+ * @use_one_entry: when true will produce ipc dma chain page table descriptor
+ *                 of the mapping in a single concurrent dma block.
+ *                 otherwise a chain of multiple blocks might be generated.
+ * @page_list: returns the dma address of the ipc dma chain page table
+ *             descriptor.
+ * @total_chunks: returns the total number of elements in the mapping's
+ *                sg_table. Can be NULL if this info is not required.
+ *
+ * This function maps the host resource to be accessible from device
+ * and returns the dma page list of DMA addresses packed in format
+ * suitable to be used in ipc level to be sent to the device.
+ *
+ * The resource can be mapped to multiple devices.
+ * The resource can be mapped to userspace and to device at the same time.
+ *
+ * Return: error on failure.
+ */
+int nnp_hostres_map_device(struct host_resource *res,
+			   struct nnp_device *nnpdev, bool use_one_entry,
+			   dma_addr_t *page_list, u32 *total_chunks);
+
+/**
+ * nnp_hostres_unmap_device() - Unmaps the host resource from NNP-I device
+ * @res: handle to host resource
+ * @nnpdev: handle to nnp device struct
+ *
+ * This function unmaps previously mapped host resource from device.
+ * The resource must be mapped to this device before calling this function.
+ * The resource must be unlocked from this device, if it was previously locked,
+ * before calling this function.
+ *
+ * Return: error on failure.
+ */
+int nnp_hostres_unmap_device(struct host_resource *res,
+			     struct nnp_device *nnpdev);
+
+/**
+ * nnp_hostres_map_user() -  Maps the host resource to userspace
+ * @res: handle to host resource
+ * @vma: user virtual memory area
+ *
+ * This function maps the host resource to userspace virtual memory.
+ * The host resource can be mapped to userspace multiple times.
+ * The host resource can be mapped to user and to device at the same time.
+ *
+ * Return: error on failure.
+ */
+int nnp_hostres_map_user(struct host_resource *res,
+			 struct vm_area_struct *vma);
+
+/**
+ * nnp_hostres_user_lock() - Lock the host resource to access from userspace
+ * @res: handle to host resource
+ *
+ * This function should be called before user-space application is accessing
+ * the host resource content (either for read or write). The function
+ * invalidates  or flashes the cpu caches when necessary.
+ * The function does *not* impose any synchronization between application and
+ * device accesses to the resource memory. Such synchronization is handled
+ * in user-space.
+ *
+ * Return: error on failure.
+ */
+int nnp_hostres_user_lock(struct host_resource *res);
+
+/**
+ * nnp_hostres_user_unlock() - Unlocks the host resource from userspace access
+ * @res: handle to host resource
+ *
+ * This function should be called after user-space application is finished
+ * accessing the host resource content (either for read or write). The function
+ * invalidates  or flashes the cpu caches when necessary.
+ *
+ * Return: error on failure.
+ */
+int nnp_hostres_user_unlock(struct host_resource *res);
+
+/**
+ * nnp_hostres_get() - Increases refcount of the hostres
+ * @res: handle to host resource
+ *
+ * This function increases refcount of the host resource.
+ */
+void nnp_hostres_get(struct host_resource *res);
+
+/**
+ * nnp_hostres_put() - Decreases refcount of the hostres
+ * @res: handle to host resource
+ *
+ * This function decreases refcount of the host resource and destroyes it
+ * when it reaches 0.
+ */
+void nnp_hostres_put(struct host_resource *res);
+
+/**
+ * nnp_hostres_is_input() - Returns if the host resource is input resource
+ * @res: handle to host resource
+ *
+ * This function returns true if the host resource can be read by device.
+ * The "input" terminology is used here since such resources are usually
+ * used as inputs to device inference network.
+ *
+ * Return: true if the reasource is readable.
+ */
+bool nnp_hostres_is_input(struct host_resource *res);
+
+/**
+ * nnp_hostres_is_output() - Returns if the host resource is output resource
+ * @res: handle to host resource
+ *
+ * This function returns true if the host resource can be modified by device.
+ * The term "output" is used here since usually such resources are used for
+ * outputs of device inference network.
+ *
+ * Return: true if the reasource is writable.
+ */
+bool nnp_hostres_is_output(struct host_resource *res);
+
+size_t nnp_hostres_get_size(struct host_resource *res);
+
+bool nnp_hostres_is_usermem(struct host_resource *res);
+
+/**
+ * nnp_hostres_vptr() - returns the virtual pointer to the resource buffer
+ * @res: handle to host resource
+ *
+ * Return: pointer to resource data or NULL if was not allocated by
+ * nnp_hostres_alloc()
+ */
+void *nnp_hostres_vptr(struct host_resource *res);
+
+int nnp_hostres_init_sysfs(struct device *dev);
+void nnp_hostres_fini_sysfs(struct device *dev);
+
+#endif
-- 
1.8.3.1


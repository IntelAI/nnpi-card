From 182e896293d3d9abcbc9a6e6f6dc73d3f344dc05 Mon Sep 17 00:00:00 2001
From: Guy Zadicario <guy.zadicario@intel.com>
Date: Sun, 15 Nov 2020 11:47:11 +0200
Subject: [PATCH v15 14/29] misc: nnpi: Map host resources to device channel

Provide an IOCTL interface for mapping and unmapping host resources to
a channel, through the device's /dev/nnpi%d char device. The mapping
gets a uniqueue ID and the page table of the host resource is
transferred to the device. Later, commands to the device can reference
the resource by the channel ID and map ID.

There is a special interface to map host resources which serve as
host-to-card and card-to-host ring buffers. These ring buffers can be
referenced later by the ring-buffer direction and index, rather than by
a map ID.

Signed-off-by: Guy Zadicario <guy.zadicario@intel.com>
---
 drivers/misc/intel-nnpi/cmd_chan.c       | 105 ++++++++
 drivers/misc/intel-nnpi/cmd_chan.h       |  36 +++
 drivers/misc/intel-nnpi/device.c         |  50 ++++
 drivers/misc/intel-nnpi/device_chardev.c | 428 +++++++++++++++++++++++++++++++
 include/uapi/misc/intel_nnpi.h           | 111 ++++++++
 5 files changed, 730 insertions(+)

diff --git a/drivers/misc/intel-nnpi/cmd_chan.c b/drivers/misc/intel-nnpi/cmd_chan.c
index b177928..dc1b793 100644
--- a/drivers/misc/intel-nnpi/cmd_chan.c
+++ b/drivers/misc/intel-nnpi/cmd_chan.c
@@ -350,6 +350,11 @@ int nnpdev_chan_create(struct nnp_device *nnpdev, int host_fd,
 	init_waitqueue_head(&cmd_chan->resp_waitq);
 	mutex_init(&cmd_chan->dev_mutex);
 
+	spin_lock_init(&cmd_chan->map_lock);
+	mutex_init(&cmd_chan->mutex);
+	ida_init(&cmd_chan->hostres_map_ida);
+	hash_init(cmd_chan->hostres_hash);
+
 	/*
 	 * Add channel to the channel hash
 	 */
@@ -382,6 +387,11 @@ static void nnp_chan_release(struct kref *kref)
 
 	nnp_chan_disconnect(cmd_chan, false);
 
+	/*
+	 * cmd_chan->hostres_map_ida is empty at this point,
+	 * calling ida_destroy is not needed
+	 */
+
 	if (cmd_chan->fd < 0)
 		fput(cmd_chan->host_file);
 
@@ -505,6 +515,9 @@ int nnp_chan_send_destroy(struct nnp_chan *chan)
 void nnp_chan_disconnect(struct nnp_chan *cmd_chan, bool dev_locked)
 {
 	struct nnp_device *nnpdev;
+	struct chan_hostres_map *hostres_map;
+	struct hlist_node *tmp;
+	int i;
 
 	mutex_lock(&cmd_chan->dev_mutex);
 	if (!cmd_chan->nnpdev) {
@@ -542,6 +555,33 @@ void nnp_chan_disconnect(struct nnp_chan *cmd_chan, bool dev_locked)
 
 	ida_simple_remove(&nnpdev->cmd_chan_ida,
 			  cmd_chan->chan_id);
+
+	/*
+	 * Unmap ring buffers
+	 */
+	for (i = 0; i < NNP_IPC_MAX_CHANNEL_RB; i++) {
+		if (cmd_chan->h2c_rb_hostres_map[i]) {
+			nnp_hostres_unmap_device(cmd_chan->h2c_rb_hostres_map[i]);
+			cmd_chan->h2c_rb_hostres_map[i] = NULL;
+		}
+		if (cmd_chan->c2h_rb_hostres_map[i]) {
+			nnp_hostres_unmap_device(cmd_chan->c2h_rb_hostres_map[i]);
+			cmd_chan->c2h_rb_hostres_map[i] = NULL;
+		}
+	}
+
+	/*
+	 * Destroy all host resource maps
+	 */
+	spin_lock(&cmd_chan->map_lock);
+	hash_for_each_safe(cmd_chan->hostres_hash, i,
+			   tmp, hostres_map, hash_node) {
+		hash_del(&hostres_map->hash_node);
+		ida_simple_remove(&cmd_chan->hostres_map_ida, hostres_map->id);
+		nnp_hostres_unmap_device(hostres_map->hostres_map);
+		kfree(hostres_map);
+	}
+	spin_unlock(&cmd_chan->map_lock);
 }
 
 static int lock_and_resize_respq(struct nnp_chan *cmd_chan)
@@ -629,3 +669,68 @@ int nnp_chan_add_response(struct nnp_chan *cmd_chan, u64 *hw_msg, u32 size)
 
 	return 0;
 }
+
+int nnp_chan_set_ringbuf(struct nnp_chan *chan, bool h2c, unsigned int id,
+			 struct nnpdev_mapping *hostres_map)
+{
+	if (id >= NNP_IPC_MAX_CHANNEL_RB)
+		return -EINVAL;
+
+	mutex_lock(&chan->dev_mutex);
+	if (h2c) {
+		if (chan->h2c_rb_hostres_map[id])
+			nnp_hostres_unmap_device(chan->h2c_rb_hostres_map[id]);
+		chan->h2c_rb_hostres_map[id] = hostres_map;
+	} else {
+		if (chan->c2h_rb_hostres_map[id])
+			nnp_hostres_unmap_device(chan->c2h_rb_hostres_map[id]);
+		chan->c2h_rb_hostres_map[id] = hostres_map;
+	}
+	mutex_unlock(&chan->dev_mutex);
+
+	return 0;
+}
+
+static struct chan_hostres_map *find_del_map(struct nnp_chan *chan, u16 map_id,
+					     bool del)
+{
+	struct chan_hostres_map *hostres_map;
+
+	spin_lock(&chan->map_lock);
+	hash_for_each_possible(chan->hostres_hash, hostres_map, hash_node,
+			       map_id)
+		if (hostres_map->id == map_id) {
+			if (del) {
+				hash_del(&hostres_map->hash_node);
+				ida_simple_remove(&chan->hostres_map_ida,
+						  hostres_map->id);
+			}
+			spin_unlock(&chan->map_lock);
+			return hostres_map;
+		}
+	spin_unlock(&chan->map_lock);
+
+	return NULL;
+}
+
+struct chan_hostres_map *nnp_chan_find_map(struct nnp_chan *chan, u16 map_id)
+{
+	return find_del_map(chan, map_id, false);
+}
+
+int nnp_chan_unmap_hostres(struct nnp_chan *chan, u16 map_id)
+{
+	struct chan_hostres_map *hostres_map;
+
+	hostres_map = find_del_map(chan, map_id, true);
+	if (!hostres_map)
+		return -ENXIO;
+
+	mutex_lock(&chan->dev_mutex);
+	nnp_hostres_unmap_device(hostres_map->hostres_map);
+	mutex_unlock(&chan->dev_mutex);
+
+	kfree(hostres_map);
+
+	return 0;
+}
diff --git a/drivers/misc/intel-nnpi/cmd_chan.h b/drivers/misc/intel-nnpi/cmd_chan.h
index 2a44998..9e13288 100644
--- a/drivers/misc/intel-nnpi/cmd_chan.h
+++ b/drivers/misc/intel-nnpi/cmd_chan.h
@@ -36,11 +36,19 @@
  * @dev_mutex: protects @nnpdev and @destroyed
  * @destroyed: a state indicating that the channel should be treated as
  *             no-longer-exist on the card.
+ * @mutex: protects card synchronous operations which modify @event_msg
+ * @hostres_map_ida: generate ipc ids for hostres mapping
+ * @map_lock: protects @hostres_hash, @hostres_map_ida
+ * @hostres_hash: hash table to store all host resource mapping, key is ipc id
  * @resp_waitq: waitqueue used for waiting for response messages be available.
  * @respq: circular buffer object that receive response messages from device.
  * @respq_lock: protects @respq
  * @respq_buf: buffer space allocated for circular response buffer.
  * @respq_size: current allocated size of circular response buffer.
+ * @h2c_rb_hostres_map: host resource mapping used for each host-to-card ring buffer
+ *                  There may be up to 2 such ring buffers, both can be NULL.
+ * @c2h_rb_hostres_map: host resource mapping used for each card-to-host ring buffer
+ *                  There may be up to 2 such ring buffers, both can be NULL.
  */
 struct nnp_chan {
 	struct kref            ref;
@@ -60,10 +68,33 @@ struct nnp_chan {
 	wait_queue_head_t resp_waitq;
 	bool              destroyed;
 
+	struct mutex      mutex;
+	struct ida        hostres_map_ida;
+	spinlock_t        map_lock;
+	DECLARE_HASHTABLE(hostres_hash, 6);
+
 	struct circ_buf   respq;
 	spinlock_t        respq_lock;
 	char             *respq_buf;
 	unsigned int      respq_size;
+
+	struct nnpdev_mapping *h2c_rb_hostres_map[NNP_IPC_MAX_CHANNEL_RB];
+	struct nnpdev_mapping *c2h_rb_hostres_map[NNP_IPC_MAX_CHANNEL_RB];
+};
+
+/**
+ * struct chan_hostres_map - holds host resource mapping to channel
+ *
+ * @id: ipc map id of the mapping
+ * @hash_node: node to include this mapping in @hostres_hash of nnpdrv_cmd_chan
+ * @hostres: the mapped host resource
+ * @event_msg: device response to the map create request
+ */
+struct chan_hostres_map {
+	u16 id;
+	struct hlist_node           hash_node;
+	struct nnpdev_mapping       *hostres_map;
+	union c2h_event_report      event_msg;
 };
 
 #define chan_broken(chan) ((chan)->card_critical_error.event_code)
@@ -83,4 +114,9 @@ int nnpdev_chan_create(struct nnp_device *nnpdev, int host_fd,
 
 int nnp_chan_add_response(struct nnp_chan *cmd_chan, u64 *hw_msg, u32 size);
 
+int nnp_chan_set_ringbuf(struct nnp_chan *chan, bool h2c, unsigned int id,
+			 struct nnpdev_mapping *hostres_map);
+
+struct chan_hostres_map *nnp_chan_find_map(struct nnp_chan *chan, u16 map_id);
+int nnp_chan_unmap_hostres(struct nnp_chan *chan, u16 map_id);
 #endif
diff --git a/drivers/misc/intel-nnpi/device.c b/drivers/misc/intel-nnpi/device.c
index 18f03d6..9220327 100644
--- a/drivers/misc/intel-nnpi/device.c
+++ b/drivers/misc/intel-nnpi/device.c
@@ -257,6 +257,41 @@ static void handle_channel_create_response(struct nnp_device *nnpdev,
 	wake_up_all(&nnpdev->waitq);
 }
 
+static void handle_channel_map_hostres(struct nnp_device *nnpdev,
+				       union c2h_event_report *event_msg)
+{
+	struct nnp_chan *cmd_chan;
+	struct chan_hostres_map *hostres_map;
+
+	cmd_chan = nnpdev_find_channel(nnpdev, event_msg->obj_id);
+	if (!cmd_chan)
+		return;
+
+	hostres_map = nnp_chan_find_map(cmd_chan, event_msg->obj_id_2);
+	if (!hostres_map)
+		goto put_chan;
+
+	hostres_map->event_msg.value = event_msg->value;
+	wake_up_all(&nnpdev->waitq);
+
+put_chan:
+	nnp_chan_put(cmd_chan);
+}
+
+static void handle_channel_unmap_hostres(struct nnp_device *nnpdev,
+					 union c2h_event_report *event_msg)
+{
+	struct nnp_chan *cmd_chan;
+
+	cmd_chan = nnpdev_find_channel(nnpdev, event_msg->obj_id);
+	if (!cmd_chan)
+		return;
+
+	nnp_chan_unmap_hostres(cmd_chan, event_msg->obj_id_2);
+
+	nnp_chan_put(cmd_chan);
+}
+
 static void handle_channel_destroy(struct nnp_device *nnpdev,
 				   union c2h_event_report *event_msg)
 {
@@ -299,8 +334,14 @@ static void process_device_event(struct nnp_device *nnpdev,
 		switch (event_msg->event_code) {
 		case NNP_IPC_CREATE_CHANNEL_SUCCESS:
 		case NNP_IPC_CREATE_CHANNEL_FAILED:
+		case NNP_IPC_CHANNEL_SET_RB_SUCCESS:
+		case NNP_IPC_CHANNEL_SET_RB_FAILED:
 			handle_channel_create_response(nnpdev, event_msg);
 			break;
+		case NNP_IPC_CHANNEL_MAP_HOSTRES_SUCCESS:
+		case NNP_IPC_CHANNEL_MAP_HOSTRES_FAILED:
+			handle_channel_map_hostres(nnpdev, event_msg);
+			break;
 		case NNP_IPC_DESTROY_CHANNEL_FAILED:
 			dev_err(nnpdev->dev,
 				"Channel destroyed failed channel %d val %d\n",
@@ -309,6 +350,15 @@ static void process_device_event(struct nnp_device *nnpdev,
 		case NNP_IPC_CHANNEL_DESTROYED:
 			handle_channel_destroy(nnpdev, event_msg);
 			break;
+		case NNP_IPC_CHANNEL_UNMAP_HOSTRES_FAILED:
+			dev_dbg(nnpdev->dev,
+				"Channel hostres unmap failed on device channel %d map %d val %d\n",
+				event_msg->obj_id, event_msg->obj_id_2,
+				event_msg->event_val);
+			fallthrough;
+		case NNP_IPC_CHANNEL_UNMAP_HOSTRES_SUCCESS:
+			handle_channel_unmap_hostres(nnpdev, event_msg);
+			break;
 		default:
 			dev_err(nnpdev->dev,
 				"Unknown event received - %u\n",
diff --git a/drivers/misc/intel-nnpi/device_chardev.c b/drivers/misc/intel-nnpi/device_chardev.c
index 5a4ec8a..fbb94e2 100644
--- a/drivers/misc/intel-nnpi/device_chardev.c
+++ b/drivers/misc/intel-nnpi/device_chardev.c
@@ -6,6 +6,7 @@
 #include <linux/bitfield.h>
 #include <linux/cdev.h>
 #include <linux/device.h>
+#include <linux/dma-map-ops.h>
 #include <linux/list.h>
 #include <linux/printk.h>
 #include <linux/slab.h>
@@ -14,6 +15,7 @@
 
 #include "cmd_chan.h"
 #include "device_chardev.h"
+#include "nnp_user.h"
 #include "ipc_c2h_events.h"
 
 static dev_t       devnum;
@@ -229,6 +231,418 @@ static long create_channel(struct device_client_info *cinfo, void __user *arg,
 	return ret;
 }
 
+/**
+ * send_rb_op() - sends CHANNEL_RB_OP command and wait for reply
+ * @chan: the command channel
+ * @rb_op_cmd: the command to send
+ * @o_errno: returns zero or error code from device
+ *
+ * The function sends a "ring buffer operation" command to the device
+ * to either create or destroy a ring buffer object.
+ * This is a synchronous operation, the function will wait until a response
+ * from the device is arrived.
+ * Return:
+ * * -EPIPE: The channel is in critical error state or sending the command
+ *           has failed.
+ * * 0: The command has sent successfully, the operation status is updated
+ *      in o_errno, if o_errno is zero, then the create/destoy operation has
+ *      been succeeded, otherwise it indicates an error code received from
+ *      device.
+ */
+static int send_rb_op(struct nnp_chan *chan, u64 rb_op_cmd,
+		      __u32 *o_errno)
+{
+	int ret = -EPIPE;
+
+	*o_errno = 0;
+
+	mutex_lock(&chan->mutex);
+	chan->event_msg.value = 0;
+
+	/* send the command to card */
+	if (!is_card_fatal_drv_event(chan->card_critical_error.event_code))
+		ret = nnpdev_queue_msg(chan->nnpdev->cmdq, rb_op_cmd);
+
+	if (ret < 0)
+		goto done;
+
+	/* wait until card respond or card critical error is detected */
+	wait_event(chan->nnpdev->waitq,
+		   chan->event_msg.value || chan_drv_fatal(chan));
+	if (!chan->event_msg.value) {
+		*o_errno = NNPER_DEVICE_ERROR;
+		ret = 0;
+		goto done;
+	}
+
+	if (chan->event_msg.event_code == NNP_IPC_CHANNEL_SET_RB_FAILED) {
+		*o_errno = event_val_to_nnp_error(chan->event_msg.event_val);
+		ret = 0;
+	}
+
+done:
+	mutex_unlock(&chan->mutex);
+	return ret;
+}
+
+static long create_channel_data_ringbuf(struct device_client_info *cinfo,
+					void __user *arg, unsigned int size)
+{
+	struct nnp_device *nnpdev = cinfo->nnpdev;
+	struct ioctl_nnpi_create_channel_data_ringbuf req;
+	struct nnp_chan *chan = NULL;
+	struct user_hostres *hostres_entry = NULL;
+	struct host_resource *hostres;
+	struct nnpdev_mapping *hostres_map;
+	u64 rb_op_cmd;
+	unsigned long dma_pfn;
+	struct nnp_user_info *nnp_user = NULL;
+	dma_addr_t page_list;
+	int ret = 0;
+	unsigned int io_size = sizeof(req);
+
+	/* only single size structure is currently supported */
+	if (size != io_size)
+		return -EINVAL;
+
+	if (copy_from_user(&req, arg, io_size))
+		return -EFAULT;
+
+	/* we have one bit in ipc protocol for ringbuf id for each direction */
+	if (req.i_id > 1)
+		return -EINVAL;
+
+	/* o_errno must be cleared on entry */
+	if (req.o_errno)
+		return -EINVAL;
+
+	chan = nnpdev_find_channel(nnpdev, req.i_channel_id);
+	if (!chan) {
+		req.o_errno = NNPER_NO_SUCH_CHANNEL;
+		goto do_exit;
+	}
+
+	nnp_user = chan->nnp_user;
+	mutex_lock(&nnp_user->mutex);
+	hostres_entry = idr_find(&nnp_user->idr, req.i_hostres_handle);
+	if (!hostres_entry) {
+		req.o_errno = NNPER_NO_SUCH_RESOURCE;
+		mutex_unlock(&nnp_user->mutex);
+		goto put_chan;
+	}
+
+	hostres = hostres_entry->hostres;
+
+	/* check the resource fit the direction */
+	if ((req.i_h2c && !nnp_hostres_is_input(hostres)) ||
+	    (!req.i_h2c && !nnp_hostres_is_output(hostres))) {
+		req.o_errno = NNPER_INCOMPATIBLE_RESOURCES;
+		mutex_unlock(&nnp_user->mutex);
+		goto put_chan;
+	}
+
+	hostres_map = nnp_hostres_map_device(hostres, nnpdev, false, &page_list, NULL);
+	if (IS_ERR(hostres_map)) {
+		ret = -EFAULT;
+		mutex_unlock(&nnp_user->mutex);
+		goto put_chan;
+	}
+
+	/*
+	 * Its OK to release the mutex here and let other
+	 * thread destroy the hostres handle as we already
+	 * mapped it (which ref counted)
+	 */
+	mutex_unlock(&nnp_user->mutex);
+
+	dma_pfn = NNP_IPC_DMA_ADDR_TO_PFN(page_list);
+	rb_op_cmd = FIELD_PREP(NNP_H2C_OP_MASK, NNP_IPC_H2C_OP_CHANNEL_RB_OP);
+	rb_op_cmd |= FIELD_PREP(NNP_H2C_CHANNEL_RB_OP_CHAN_ID_MASK,
+				chan->chan_id);
+	rb_op_cmd |= FIELD_PREP(NNP_H2C_CHANNEL_RB_OP_ID_MASK, req.i_id);
+	rb_op_cmd |= FIELD_PREP(NNP_H2C_CHANNEL_RB_OP_HOST_PFN_MASK, dma_pfn);
+	if (req.i_h2c)
+		rb_op_cmd |= FIELD_PREP(NNP_H2C_CHANNEL_RB_OP_H2C_MASK, 1);
+
+	ret = send_rb_op(chan, rb_op_cmd, &req.o_errno);
+	if (ret || req.o_errno)
+		goto err_hostres_map;
+
+	ret = nnp_chan_set_ringbuf(chan, req.i_h2c, req.i_id, hostres_map);
+
+	if (ret == 0)
+		goto put_chan;
+
+err_hostres_map:
+	nnp_hostres_unmap_device(hostres_map);
+put_chan:
+	nnp_chan_put(chan);
+do_exit:
+	if (!ret && copy_to_user(arg, &req, io_size))
+		return -EFAULT;
+
+	return ret;
+}
+
+static long destroy_channel_data_ringbuf(struct device_client_info *cinfo,
+					 void __user *arg, unsigned int size)
+{
+	struct nnp_device *nnpdev = cinfo->nnpdev;
+	struct ioctl_nnpi_destroy_channel_data_ringbuf req;
+	struct nnp_chan *chan;
+	u64 rb_op_cmd;
+	int ret = 0;
+	unsigned int io_size = sizeof(req);
+
+	/* only single size structure is currently supported */
+	if (size != io_size)
+		return -EINVAL;
+
+	if (copy_from_user(&req, arg, io_size))
+		return -EFAULT;
+
+	/* we have one bit in ipc protocol for ringbuf id for each direction */
+	if (req.i_id > 1)
+		return -EINVAL;
+
+	/* o_errno must be cleared on entry */
+	if (req.o_errno)
+		return -EINVAL;
+
+	chan = nnpdev_find_channel(nnpdev, req.i_channel_id);
+	if (!chan) {
+		req.o_errno = NNPER_NO_SUCH_CHANNEL;
+		goto done;
+	}
+
+	rb_op_cmd = FIELD_PREP(NNP_H2C_OP_MASK, NNP_IPC_H2C_OP_CHANNEL_RB_OP);
+	rb_op_cmd |= FIELD_PREP(NNP_H2C_CHANNEL_RB_OP_CHAN_ID_MASK,
+				chan->chan_id);
+	rb_op_cmd |= FIELD_PREP(NNP_H2C_CHANNEL_RB_OP_ID_MASK, req.i_id);
+	rb_op_cmd |= FIELD_PREP(NNP_H2C_CHANNEL_RB_OP_DESTROY_MASK, 1);
+	if (req.i_h2c)
+		rb_op_cmd |= FIELD_PREP(NNP_H2C_CHANNEL_RB_OP_H2C_MASK, 1);
+
+	ret = send_rb_op(chan, rb_op_cmd, &req.o_errno);
+	if (ret || req.o_errno)
+		goto put_chan;
+
+	ret = nnp_chan_set_ringbuf(chan, req.i_h2c, req.i_id, NULL);
+
+put_chan:
+	nnp_chan_put(chan);
+done:
+	if (!ret && copy_to_user(arg, &req, io_size))
+		return -EFAULT;
+
+	return ret;
+}
+
+static long map_hostres(struct device_client_info *cinfo,
+			void __user *arg, unsigned int size)
+{
+	struct nnp_device *nnpdev = cinfo->nnpdev;
+	struct ioctl_nnpi_channel_map_hostres req;
+	struct nnp_chan *chan = NULL;
+	struct user_hostres *hostres_entry = NULL;
+	struct host_resource *hostres;
+	struct nnp_user_info *nnp_user = NULL;
+	struct chan_hostres_map *hostres_map = NULL;
+	u64 cmd[2];
+	unsigned long dma_pfn;
+	dma_addr_t page_list;
+	int map_id;
+	int ret = 0;
+	unsigned int io_size = sizeof(req);
+	const struct dma_map_ops *ops;
+
+	/* only single size structure is currently supported */
+	if (size != io_size)
+		return -EINVAL;
+
+	if (copy_from_user(&req, arg, io_size))
+		return -EFAULT;
+
+	req.o_errno = 0;
+
+	chan = nnpdev_find_channel(nnpdev, req.i_channel_id);
+	if (!chan) {
+		req.o_errno = NNPER_NO_SUCH_CHANNEL;
+		goto do_exit;
+	}
+
+	nnp_user = chan->nnp_user;
+	mutex_lock(&nnp_user->mutex);
+	hostres_entry = idr_find(&nnp_user->idr, req.i_hostres_handle);
+	if (!hostres_entry) {
+		req.o_errno = NNPER_NO_SUCH_RESOURCE;
+		mutex_unlock(&nnp_user->mutex);
+		goto put_chan;
+	}
+	hostres = hostres_entry->hostres;
+
+	hostres_map = kzalloc(sizeof(*hostres_map), GFP_KERNEL);
+	if (!hostres_map) {
+		req.o_errno = ENOMEM;
+		mutex_unlock(&nnp_user->mutex);
+		goto put_chan;
+	}
+
+	map_id = -1;
+	spin_lock(&chan->map_lock);
+	ret = ida_simple_get(&chan->hostres_map_ida, 0, U16_MAX, GFP_KERNEL);
+	if (ret < 0) {
+		req.o_errno = ENOMEM;
+		ret = 0;
+		spin_unlock(&chan->map_lock);
+		mutex_unlock(&nnp_user->mutex);
+		goto err_map;
+	}
+	map_id = ret;
+	spin_unlock(&chan->map_lock);
+
+	hostres_map->hostres_map = nnp_hostres_map_device(hostres, nnpdev,
+							  false, &page_list, NULL);
+	if (IS_ERR(hostres_map->hostres_map)) {
+		ret = -EFAULT;
+		mutex_unlock(&nnp_user->mutex);
+		goto err_ida;
+	}
+
+	/*
+	 * Its OK to release the mutex here and let other
+	 * thread destroy the hostres handle as we already
+	 * mapped it (which ref counted)
+	 */
+	mutex_unlock(&nnp_user->mutex);
+
+	dma_pfn = NNP_IPC_DMA_ADDR_TO_PFN(page_list);
+	cmd[0] = FIELD_PREP(NNP_H2C_OP_MASK, NNP_IPC_H2C_OP_CHANNEL_HOSTRES_OP);
+	cmd[0] |= FIELD_PREP(NNP_H2C_CHANNEL_HOSTRES_QW0_CHAN_ID_MASK,
+			     chan->chan_id);
+	cmd[0] |= FIELD_PREP(NNP_H2C_CHANNEL_HOSTRES_QW0_ID_MASK, map_id);
+	cmd[1] = FIELD_PREP(NNP_H2C_CHANNEL_HOSTRES_QW1_HOST_PFN_MASK, dma_pfn);
+
+	hostres_map->event_msg.value = 0;
+	hostres_map->id = (u16)map_id;
+
+	spin_lock(&chan->map_lock);
+	hash_add(chan->hostres_hash,
+		 &hostres_map->hash_node,
+		 hostres_map->id);
+	spin_unlock(&chan->map_lock);
+
+	/* send the hostres map command to card */
+	ret = -EPIPE;
+	if (!chan_drv_fatal(chan))
+		ret = nnpdev_queue_msg(chan->cmdq, cmd);
+	if (ret < 0) {
+		req.o_errno = NNPER_DEVICE_ERROR;
+		ret = 0;
+		goto err_hostres_map;
+	}
+
+	/* wait until card respond or card critical error is detected */
+	wait_event(nnpdev->waitq,
+		   hostres_map->event_msg.value || chan_drv_fatal(chan));
+
+	if (!hostres_map->event_msg.value) {
+		req.o_errno = NNPER_DEVICE_ERROR;
+		ret = 0;
+		goto err_hostres_map;
+	}
+
+	if (hostres_map->event_msg.event_code ==
+	    NNP_IPC_CHANNEL_MAP_HOSTRES_FAILED) {
+		req.o_errno =
+		  event_val_to_nnp_error(hostres_map->event_msg.event_val);
+		ret = 0;
+		goto err_hostres_map;
+	}
+
+	if (ret)
+		goto err_hostres_map;
+
+	ops = get_dma_ops(nnpdev->dev);
+	if (ops)
+		req.o_sync_needed = (ops->sync_sg_for_cpu ? 1 : 0);
+	else
+		req.o_sync_needed =
+			!dev_is_dma_coherent(nnpdev->dev);
+
+	req.o_map_id = (u16)map_id;
+
+	goto put_chan;
+
+err_hostres_map:
+	nnp_chan_unmap_hostres(chan, (u16)map_id);
+err_ida:
+	spin_lock(&chan->map_lock);
+	ida_simple_remove(&chan->hostres_map_ida, map_id);
+	spin_unlock(&chan->map_lock);
+err_map:
+	kfree(hostres_map);
+put_chan:
+	nnp_chan_put(chan);
+do_exit:
+	if (!ret && copy_to_user(arg, &req, io_size))
+		ret = -EFAULT;
+
+	return ret;
+}
+
+static long unmap_hostres(struct device_client_info *cinfo, void __user *arg,
+			  unsigned int size)
+{
+	struct nnp_device *nnpdev = cinfo->nnpdev;
+	struct ioctl_nnpi_channel_unmap_hostres req;
+	struct nnp_chan *chan = NULL;
+	struct chan_hostres_map *hostres_map;
+	u64 cmd[2];
+	long ret = 0;
+	unsigned int io_size = sizeof(req);
+
+	/* only single size structure is currently supported */
+	if (size != io_size)
+		return -EINVAL;
+
+	if (copy_from_user(&req, arg, io_size))
+		return -EFAULT;
+
+	/* o_errno must be cleared on entry */
+	if (req.o_errno)
+		return -EINVAL;
+
+	chan = nnpdev_find_channel(nnpdev, req.i_channel_id);
+	if (!chan) {
+		req.o_errno = NNPER_NO_SUCH_CHANNEL;
+		goto done;
+	}
+
+	hostres_map = nnp_chan_find_map(chan, req.i_map_id);
+	if (!hostres_map) {
+		req.o_errno = NNPER_NO_SUCH_HOSTRES_MAP;
+		goto put_chan;
+	}
+
+	cmd[0] = FIELD_PREP(NNP_H2C_OP_MASK, NNP_IPC_H2C_OP_CHANNEL_HOSTRES_OP);
+	cmd[0] |= FIELD_PREP(NNP_H2C_CHANNEL_HOSTRES_QW0_CHAN_ID_MASK,
+			     chan->chan_id);
+	cmd[0] |= FIELD_PREP(NNP_H2C_CHANNEL_HOSTRES_QW0_ID_MASK, req.i_map_id);
+	cmd[0] |= FIELD_PREP(NNP_H2C_CHANNEL_HOSTRES_QW0_UNMAP_MASK, 1);
+	cmd[1] = 0;
+
+	ret = nnpdev_queue_msg(chan->cmdq, cmd);
+
+put_chan:
+	nnp_chan_put(chan);
+done:
+	if (!ret && copy_to_user(arg, &req, io_size))
+		return -EFAULT;
+
+	return ret;
+}
+
 static long nnp_device_ioctl(struct file *f, unsigned int cmd,
 			     unsigned long arg)
 {
@@ -255,6 +669,20 @@ static long nnp_device_ioctl(struct file *f, unsigned int cmd,
 	case _IOC_NR(IOCTL_NNPI_DEVICE_CREATE_CHANNEL):
 		ret = create_channel(client, (void __user *)arg, size);
 		break;
+	case _IOC_NR(IOCTL_NNPI_DEVICE_CREATE_CHANNEL_RB):
+		ret = create_channel_data_ringbuf(client, (void __user *)arg,
+						  size);
+		break;
+	case _IOC_NR(IOCTL_NNPI_DEVICE_DESTROY_CHANNEL_RB):
+		ret = destroy_channel_data_ringbuf(client, (void __user *)arg,
+						   size);
+		break;
+	case _IOC_NR(IOCTL_NNPI_DEVICE_CHANNEL_MAP_HOSTRES):
+		ret = map_hostres(client, (void __user *)arg, size);
+		break;
+	case _IOC_NR(IOCTL_NNPI_DEVICE_CHANNEL_UNMAP_HOSTRES):
+		ret = unmap_hostres(client, (void __user *)arg, size);
+		break;
 	default:
 		ret = -EINVAL;
 		break;
diff --git a/include/uapi/misc/intel_nnpi.h b/include/uapi/misc/intel_nnpi.h
index 5555efe..b735361 100644
--- a/include/uapi/misc/intel_nnpi.h
+++ b/include/uapi/misc/intel_nnpi.h
@@ -150,6 +150,43 @@ struct nnpdrv_ioctl_destroy_hostres {
 	_IOWR('D', 0, struct ioctl_nnpi_create_channel)
 
 /**
+ * IOCTL_NNPI_DEVICE_CREATE_CHANNEL_RB:
+ *
+ * A request to create a data ring buffer for a command channel object.
+ * This is used to transfer data together with command to the device.
+ * A device command may include a data size fields which indicate how much data
+ * has pushed into that ring-buffer object.
+ */
+#define IOCTL_NNPI_DEVICE_CREATE_CHANNEL_RB   \
+	_IOWR('D', 1, struct ioctl_nnpi_create_channel_data_ringbuf)
+
+/**
+ * IOCTL_NNPI_DEVICE_DESTROY_CHANNEL_RB:
+ *
+ * A request to destoy a data ring buffer allocated for a command channel.
+ */
+#define IOCTL_NNPI_DEVICE_DESTROY_CHANNEL_RB  \
+	_IOWR('D', 2, struct ioctl_nnpi_destroy_channel_data_ringbuf)
+
+/**
+ * IOCTL_NNPI_DEVICE_CHANNEL_MAP_HOSTRES:
+ *
+ * A request to map a host resource to a command channel object.
+ * Device commands can include "map id" of this mapping for referencing
+ * a host resource.
+ */
+#define IOCTL_NNPI_DEVICE_CHANNEL_MAP_HOSTRES \
+	_IOWR('D', 3, struct ioctl_nnpi_channel_map_hostres)
+
+/**
+ * IOCTL_NNPI_DEVICE_CHANNEL_UNMAP_HOSTRES:
+ *
+ * A request to unmap a host resource previously mapped to a command channel.
+ */
+#define IOCTL_NNPI_DEVICE_CHANNEL_UNMAP_HOSTRES \
+	_IOWR('D', 4, struct ioctl_nnpi_channel_unmap_hostres)
+
+/**
  * struct ioctl_nnpi_create_channel - IOCTL_NNPI_DEVICE_CREATE_CHANNEL payload
  * @i_host_fd: opened file descriptor to /dev/nnpi_host
  * @i_min_id: minimum range for channel id allocation
@@ -177,6 +214,80 @@ struct ioctl_nnpi_create_channel {
 	__u16    o_channel_id;
 };
 
+/**
+ * struct ioctl_nnpi_create_channel_data_ringbuf
+ * @i_hostres_handle: handle of a host resource which will be used to hold
+ *         the ring-buffer content.
+ * @i_channel_id: command channel id.
+ * @i_id: id of the ring buffer object (can be 0 or 1).
+ * @i_h2c: non-zero if this ring-buffer is for command submission use,
+ *         otherwise it is for responses.
+ * @o_errno: On input, must be set to 0.
+ *           On output, 0 on success, one of the NNPERR_* error codes on error.
+ *
+ * this is the payload for IOCTL_NNPI_DEVICE_CREATE_CHANNEL_RB ioctl
+ */
+struct ioctl_nnpi_create_channel_data_ringbuf {
+	__s32 i_hostres_handle;
+	__u32 i_channel_id;
+	__u32 i_id;
+	__u32 i_h2c;
+	__u32 o_errno;
+};
+
+/**
+ * struct ioctl_nnpi_destroy_channel_data_ringbuf
+ * @i_channel_id: command channel id.
+ * @i_id: id of the ring buffer object (can be 0 or 1).
+ * @i_h2c: true if this ring-buffer is for command submission use,
+ *         otherwise it is for responses.
+ * @o_errno: On input, must be set to 0.
+ *           On output, 0 on success, one of the NNPERR_* error codes on error.
+ *
+ * this is the payload for IOCTL_NNPI_DEVICE_DESTROY_CHANNEL_RB ioctl
+ */
+struct ioctl_nnpi_destroy_channel_data_ringbuf {
+	__u32 i_channel_id;
+	__u32 i_id;
+	__u32 i_h2c;
+	__u32 o_errno;
+};
+
+/**
+ * struct ioctl_nnpi_channel_map_hostres
+ * @i_hostres_handle: handle of a host resource to be mapped
+ * @i_channel_id: command channel id.
+ * @o_map_id: returns unique id of the mapping
+ * @o_sync_needed: returns non-zero if LOCK/UNLOCK_HOST_RESOURCE ioctls
+ *            needs to be used before/after accessing the resource from cpu.
+ * @o_errno: On input, must be set to 0.
+ *           On output, 0 on success, one of the NNPERR_* error codes on error.
+ *
+ * this is the payload for IOCTL_NNPI_DEVICE_CHANNEL_MAP_HOSTRES ioctl
+ */
+struct ioctl_nnpi_channel_map_hostres {
+	__s32 i_hostres_handle;
+	__u32 i_channel_id;
+	__u32 o_map_id;
+	__u32 o_sync_needed;
+	__u32 o_errno;
+};
+
+/**
+ * ioctl_nnpi_channel_unmap_hostres
+ * @i_channel_id: command channel id.
+ * @i_map_id: mapping id
+ * @o_errno: On input, must be set to 0.
+ *           On output, 0 on success, one of the NNPERR_* error codes on error.
+ *
+ * This is the payload for IOCTL_NNPI_DEVICE_CHANNEL_UNMAP_HOSTRES ioctl
+ */
+struct ioctl_nnpi_channel_unmap_hostres {
+	__u32 i_channel_id;
+	__u32 i_map_id;
+	__u32 o_errno;
+};
+
 /****************************************************************
  * Error code values - errors returned in o_errno fields of
  * above structures.
-- 
1.8.3.1


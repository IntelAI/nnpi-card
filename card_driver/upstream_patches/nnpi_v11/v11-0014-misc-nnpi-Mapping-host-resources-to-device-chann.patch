From bc06e40180e2d5b300c2cfb7cf21b1cf0d65bcc9 Mon Sep 17 00:00:00 2001
From: Guy Zadicario <guy.zadicario@intel.com>
Date: Sun, 15 Nov 2020 11:47:11 +0200
Subject: [PATCH v11 14/14] misc: nnpi: Mapping host resources to device
 channel

Provide an IOCTL interface for mapping and unmapping host resources to a
channel, through the device's /dev/nnpi%d char device. The mapping gets a
uniqueue ID and the page table of the host resource is transferred to the
device. Later, commands to the device can reference the resource by the
channel ID and map ID.

There is a special interface to map host resources which serve as
host-to-card and card-to-host ring buffers. These ring buffers can be
referenced later by the ring-buffer direction and index, rather than by
a map ID.

Signed-off-by: Guy Zadicario <guy.zadicario@intel.com>
Reviewed-by: Vaibhav Agarwal <vaibhav.agarwal@intel.com>
---
 drivers/misc/intel-nnpi/cmd_chan.c       |  89 +++++++
 drivers/misc/intel-nnpi/cmd_chan.h       |  39 +++
 drivers/misc/intel-nnpi/device.c         |  50 ++++
 drivers/misc/intel-nnpi/device_chardev.c | 421 +++++++++++++++++++++++++++++++
 include/uapi/misc/intel_nnpi.h           | 111 ++++++++
 5 files changed, 710 insertions(+)

diff --git a/drivers/misc/intel-nnpi/cmd_chan.c b/drivers/misc/intel-nnpi/cmd_chan.c
index dc42195..59b639c 100644
--- a/drivers/misc/intel-nnpi/cmd_chan.c
+++ b/drivers/misc/intel-nnpi/cmd_chan.c
@@ -335,6 +335,11 @@ int nnpdev_chan_create(struct nnp_device *nnpdev, int host_fd,
 	init_waitqueue_head(&cmd_chan->resp_waitq);
 	spin_lock_init(&cmd_chan->resp_lock_bh);
 
+	spin_lock_init(&cmd_chan->lock);
+	mutex_init(&cmd_chan->mutex);
+	ida_init(&cmd_chan->hostres_map_ida);
+	hash_init(cmd_chan->hostres_hash);
+
 	/*
 	 * Add channel to the channel hash
 	 */
@@ -363,6 +368,9 @@ static void nnp_chan_release(struct kref *kref)
 {
 	struct nnp_chan *cmd_chan;
 	struct nnp_device *nnpdev;
+	struct chan_hostres_map *hostres_map;
+	struct hlist_node *tmp;
+	int i;
 
 	cmd_chan = container_of(kref, struct nnp_chan, ref);
 	nnpdev = cmd_chan->nnpdev;
@@ -376,6 +384,24 @@ static void nnp_chan_release(struct kref *kref)
 	ida_simple_remove(&cmd_chan->nnpdev->cmd_chan_ida,
 			  cmd_chan->chan_id);
 
+	/*
+	 * destroy all host resource maps
+	 */
+	hash_for_each_safe(cmd_chan->hostres_hash, i,
+			   tmp, hostres_map, hash_node) {
+		hash_del(&hostres_map->hash_node);
+		ida_simple_remove(&cmd_chan->hostres_map_ida, hostres_map->id);
+		nnp_hostres_unmap_device(hostres_map->hostres, nnpdev);
+		kfree(hostres_map);
+	}
+
+	ida_destroy(&cmd_chan->hostres_map_ida);
+
+	for (i = 0; i < NNP_IPC_MAX_CHANNEL_RB; i++) {
+		nnp_chan_set_ringbuf(cmd_chan, true, i, NULL);
+		nnp_chan_set_ringbuf(cmd_chan, false, i, NULL);
+	}
+
 	if (cmd_chan->fd < 0)
 		fput(cmd_chan->host_file);
 
@@ -533,3 +559,66 @@ int nnp_chan_add_response(struct nnp_chan *cmd_chan, u64 *hw_msg, u32 size)
 
 	return 0;
 }
+
+int nnp_chan_set_ringbuf(struct nnp_chan *chan, bool h2c, unsigned int id,
+			 struct host_resource *hostres)
+{
+	if (id >= NNP_IPC_MAX_CHANNEL_RB)
+		return -EINVAL;
+
+	if (h2c) {
+		if (chan->h2c_rb_hostres[id])
+			nnp_hostres_unmap_device(chan->h2c_rb_hostres[id],
+						 chan->nnpdev);
+		chan->h2c_rb_hostres[id] = hostres;
+	} else {
+		if (chan->c2h_rb_hostres[id])
+			nnp_hostres_unmap_device(chan->c2h_rb_hostres[id],
+						 chan->nnpdev);
+		chan->c2h_rb_hostres[id] = hostres;
+	}
+
+	return 0;
+}
+
+struct chan_hostres_map *nnp_chan_find_map(struct nnp_chan *chan, u16 map_id)
+{
+	struct chan_hostres_map *hostres_map;
+
+	spin_lock(&chan->lock);
+	hash_for_each_possible(chan->hostres_hash, hostres_map, hash_node,
+			       map_id)
+		if (hostres_map->id == map_id) {
+			spin_unlock(&chan->lock);
+			return hostres_map;
+		}
+	spin_unlock(&chan->lock);
+
+	return NULL;
+}
+
+int nnp_chan_unmap_hostres(struct nnp_chan *chan, u16 map_id)
+{
+	struct chan_hostres_map *hostres_map;
+	bool found = false;
+
+	spin_lock(&chan->lock);
+	hash_for_each_possible(chan->hostres_hash, hostres_map, hash_node,
+			       map_id)
+		if (hostres_map->id == map_id) {
+			found = true;
+			hash_del(&hostres_map->hash_node);
+			ida_simple_remove(&chan->hostres_map_ida,
+					  hostres_map->id);
+			break;
+		}
+	spin_unlock(&chan->lock);
+
+	if (!found)
+		return -ENXIO;
+
+	nnp_hostres_unmap_device(hostres_map->hostres, chan->nnpdev);
+	kfree(hostres_map);
+
+	return 0;
+}
diff --git a/drivers/misc/intel-nnpi/cmd_chan.h b/drivers/misc/intel-nnpi/cmd_chan.h
index 1c25b10a..e9e1d8f 100644
--- a/drivers/misc/intel-nnpi/cmd_chan.h
+++ b/drivers/misc/intel-nnpi/cmd_chan.h
@@ -11,9 +11,12 @@
 #include <linux/hashtable.h>
 #include <linux/kref.h>
 #include <linux/list.h>
+#include <linux/mutex.h>
+#include <linux/spinlock.h>
 #include <linux/spinlock.h>
 #include <linux/wait.h>
 #include "device.h"
+#include "hostres.h"
 #include "ipc_protocol.h"
 #include "nnp_user.h"
 
@@ -39,9 +42,17 @@
  * @resp_lock_bh: protects accesses to @respq and @destroyed
  * @destroyed: a state indicating that the channel should be not-yet-exist on
  *             the device.
+ * @mutex: protects card synchronous operations which modify @event_msg
+ * @hostres_map_ida: generate ipc ids for hostres mapping
+ * @lock: protects @hostres_hash, @hostres_map_ida
+ * @hostres_hash: hash table to store all host resource mapping, key is ipc id
  * @resp_waitq: waitqueue used for waiting for response messages be available.
  * @respq: circular buffer object that receive response messages from device.
  * @respq_buf: buffer space allocated for circular response buffer.
+ * @h2c_rb_hostres: host resource used for each host-to-card ring buffer
+ *                  There may be up to 2 such ring buffers, both can be NULL.
+ * @c2h_rb_hostres: host resource used for each card-to-host ring buffer
+ *                  There may be up to 2 such ring buffers, both can be NULL.
  */
 struct nnp_chan {
 	struct kref            ref;
@@ -61,8 +72,31 @@ struct nnp_chan {
 	bool              destroyed;
 
 	spinlock_t        resp_lock_bh; /* protects @respq, @destroyed */
+	struct mutex      mutex;
+	struct ida        hostres_map_ida;
+	spinlock_t        lock; /* protects @hostres_hash, @hostres_map_ida */
+	DECLARE_HASHTABLE(hostres_hash, 6);
+
 	struct circ_buf   respq;
 	char             *respq_buf;
+
+	struct host_resource *h2c_rb_hostres[NNP_IPC_MAX_CHANNEL_RB];
+	struct host_resource *c2h_rb_hostres[NNP_IPC_MAX_CHANNEL_RB];
+};
+
+/**
+ * struct chan_hostres_map - holds host resource mapping to channel
+ *
+ * @id: ipc map id of the mapping
+ * @hash_node: node to include this mapping in @hostres_hash of nnpdrv_cmd_chan
+ * @hostres: the mapped host resource
+ * @event_msg: device response to the map create request
+ */
+struct chan_hostres_map {
+	u16 id;
+	struct hlist_node           hash_node;
+	struct host_resource *hostres;
+	union c2h_event_report      event_msg;
 };
 
 #define chan_broken(chan) ((chan)->card_critical_error.event_code)
@@ -81,4 +115,9 @@ int nnpdev_chan_create(struct nnp_device *nnpdev, int host_fd,
 
 int nnp_chan_add_response(struct nnp_chan *cmd_chan, u64 *hw_msg, u32 size);
 
+int nnp_chan_set_ringbuf(struct nnp_chan *chan, bool h2c, unsigned int id,
+			 struct host_resource *hostres);
+
+struct chan_hostres_map *nnp_chan_find_map(struct nnp_chan *chan, u16 map_id);
+int nnp_chan_unmap_hostres(struct nnp_chan *chan, u16 map_id);
 #endif
diff --git a/drivers/misc/intel-nnpi/device.c b/drivers/misc/intel-nnpi/device.c
index ab5ce6c..c54687f 100644
--- a/drivers/misc/intel-nnpi/device.c
+++ b/drivers/misc/intel-nnpi/device.c
@@ -317,6 +317,41 @@ static void handle_channel_create_response(struct nnp_device *nnpdev,
 	wake_up_all(&nnpdev->waitq);
 }
 
+static void handle_channel_map_hostres(struct nnp_device *nnpdev,
+				       union c2h_event_report *event_msg)
+{
+	struct nnp_chan *cmd_chan;
+	struct chan_hostres_map *hostres_map;
+
+	cmd_chan = nnpdev_find_channel(nnpdev, event_msg->obj_id);
+	if (!cmd_chan)
+		return;
+
+	hostres_map = nnp_chan_find_map(cmd_chan, event_msg->obj_id_2);
+	if (!hostres_map)
+		goto put_chan;
+
+	hostres_map->event_msg.value = event_msg->value;
+	wake_up_all(&nnpdev->waitq);
+
+put_chan:
+	nnp_chan_put(cmd_chan);
+}
+
+static void handle_channel_unmap_hostres(struct nnp_device *nnpdev,
+					 union c2h_event_report *event_msg)
+{
+	struct nnp_chan *cmd_chan;
+
+	cmd_chan = nnpdev_find_channel(nnpdev, event_msg->obj_id);
+	if (!cmd_chan)
+		return;
+
+	nnp_chan_unmap_hostres(cmd_chan, event_msg->obj_id_2);
+
+	nnp_chan_put(cmd_chan);
+}
+
 static void handle_channel_destroy(struct nnp_device *nnpdev,
 				   union c2h_event_report *event_msg)
 {
@@ -359,8 +394,14 @@ static void process_device_event(struct nnp_device *nnpdev,
 		switch (event_msg->event_code) {
 		case NNP_IPC_CREATE_CHANNEL_SUCCESS:
 		case NNP_IPC_CREATE_CHANNEL_FAILED:
+		case NNP_IPC_CHANNEL_SET_RB_SUCCESS:
+		case NNP_IPC_CHANNEL_SET_RB_FAILED:
 			handle_channel_create_response(nnpdev, event_msg);
 			break;
+		case NNP_IPC_CHANNEL_MAP_HOSTRES_SUCCESS:
+		case NNP_IPC_CHANNEL_MAP_HOSTRES_FAILED:
+			handle_channel_map_hostres(nnpdev, event_msg);
+			break;
 		case NNP_IPC_DESTROY_CHANNEL_FAILED:
 			dev_err(nnpdev->hw_dev->dev,
 				"Channel destroyed failed channel %d val %d\n",
@@ -369,6 +410,15 @@ static void process_device_event(struct nnp_device *nnpdev,
 		case NNP_IPC_CHANNEL_DESTROYED:
 			handle_channel_destroy(nnpdev, event_msg);
 			break;
+		case NNP_IPC_CHANNEL_UNMAP_HOSTRES_FAILED:
+			dev_dbg(nnpdev->hw_dev->dev,
+				"Channel hostres unmap failed on device channel %d map %d val %d\n",
+				event_msg->obj_id, event_msg->obj_id_2,
+				event_msg->event_val);
+			fallthrough;
+		case NNP_IPC_CHANNEL_UNMAP_HOSTRES_SUCCESS:
+			handle_channel_unmap_hostres(nnpdev, event_msg);
+			break;
 		default:
 			dev_err(nnpdev->hw_dev->dev,
 				"Unknown event received - %u\n",
diff --git a/drivers/misc/intel-nnpi/device_chardev.c b/drivers/misc/intel-nnpi/device_chardev.c
index 76f3520..803bd23 100644
--- a/drivers/misc/intel-nnpi/device_chardev.c
+++ b/drivers/misc/intel-nnpi/device_chardev.c
@@ -7,6 +7,7 @@
 #include <linux/bitfield.h>
 #include <linux/cdev.h>
 #include <linux/device.h>
+#include <linux/dma-map-ops.h>
 #include <linux/init.h>
 #include <linux/kernel.h>
 #include <linux/module.h>
@@ -87,6 +88,7 @@ static long create_channel(struct device_client_info *cinfo, void __user *arg,
 	long ret = 0;
 	unsigned int io_size = sizeof(req);
 
+	/* only single size structure is currently supported */
 	if (size != io_size)
 		return -EINVAL;
 
@@ -169,6 +171,7 @@ static long create_channel(struct device_client_info *cinfo, void __user *arg,
 	} else if (chan->event_msg.event_code ==
 		   NNP_IPC_CREATE_CHANNEL_FAILED) {
 		req.o_errno = event_val_to_nnp_error(chan->event_msg.event_val);
+		ret = 0;
 		if (!nnp_chan_set_destroyed(chan))
 			nnp_chan_put(chan);
 		goto done;
@@ -195,6 +198,414 @@ static long create_channel(struct device_client_info *cinfo, void __user *arg,
 	return ret;
 }
 
+static int send_rb_op(struct nnp_chan *chan, u64 rb_op_cmd,
+		      __u32 *o_errno)
+{
+	int ret = -EPIPE;
+
+	mutex_lock(&chan->mutex);
+	chan->event_msg.value = 0;
+
+	if (!is_card_fatal_drv_event(chan->card_critical_error.event_code))
+		ret = nnpdev_queue_msg(chan->nnpdev->cmdq, rb_op_cmd);
+
+	if (ret < 0)
+		goto done;
+
+	ret = wait_event_interruptible(chan->nnpdev->waitq,
+				       chan->event_msg.value != 0 ||
+				       chan_drv_fatal(chan));
+	if (chan->event_msg.value == 0) {
+		if (ret < 0) {
+			ret = -EINTR;
+		} else {
+			*o_errno = NNPER_DEVICE_ERROR;
+			ret = 0;
+		}
+		goto done;
+	} else if (chan->event_msg.event_code ==
+		   NNP_IPC_CHANNEL_SET_RB_FAILED) {
+		*o_errno = event_val_to_nnp_error(chan->event_msg.event_val);
+		ret = 0;
+	}
+
+done:
+	mutex_unlock(&chan->mutex);
+	return ret;
+}
+
+static long create_channel_data_ringbuf(struct device_client_info *cinfo,
+					void __user *arg, unsigned int size)
+{
+	struct nnp_device *nnpdev = cinfo->nnpdev;
+	struct ioctl_nnpi_create_channel_data_ringbuf req;
+	struct nnp_chan *chan = NULL;
+	struct user_hostres *hostres_entry = NULL;
+	struct host_resource *hostres;
+	u64 rb_op_cmd;
+	unsigned long dma_pfn;
+	struct nnp_user_info *nnp_user = NULL;
+	dma_addr_t page_list;
+	u32 total_chunks;
+	int ret;
+	unsigned int io_size = sizeof(req);
+
+	/* only single size structure is currently supported */
+	if (size != io_size)
+		return -EINVAL;
+
+	ret = copy_from_user(&req, arg, io_size);
+	if (ret != 0)
+		return -EFAULT;
+
+	if (req.i_hostres_handle >= INT_MAX)
+		return -EINVAL;
+
+	/* we have one bit in ipc protocol for ringbuf id for each direction */
+	if (req.i_id > 1)
+		return -EINVAL;
+
+	/* o_errno must be cleared on entry */
+	if (req.o_errno)
+		return -EINVAL;
+
+	chan = nnpdev_find_channel(nnpdev, req.i_channel_id);
+	if (!chan) {
+		req.o_errno = NNPER_NO_SUCH_CHANNEL;
+		goto do_exit;
+	}
+
+	nnp_user = chan->nnp_user;
+	mutex_lock(&nnp_user->mutex);
+	hostres_entry = idr_find(&nnp_user->idr,
+				 (int)req.i_hostres_handle);
+	if (!hostres_entry) {
+		req.o_errno = NNPER_NO_SUCH_RESOURCE;
+		mutex_unlock(&nnp_user->mutex);
+		goto put_chan;
+	}
+
+	hostres = hostres_entry->hostres;
+
+	/* check the resource fit the direction */
+	if ((req.i_h2c && !nnp_hostres_is_input(hostres)) ||
+	    (!req.i_h2c && !nnp_hostres_is_output(hostres))) {
+		req.o_errno = NNPER_INCOMPATIBLE_RESOURCES;
+		mutex_unlock(&nnp_user->mutex);
+		goto put_chan;
+	}
+
+	ret = nnp_hostres_map_device(hostres, nnpdev, false, &page_list,
+				     &total_chunks);
+	if (ret != 0) {
+		ret = -EFAULT;
+		mutex_unlock(&nnp_user->mutex);
+		goto put_chan;
+	}
+
+	/*
+	 * Its OK to release the mutex here and let other
+	 * thread destroy the hostres handle as we already
+	 * mapped it (which ref counted)
+	 */
+	mutex_unlock(&nnp_user->mutex);
+
+	dma_pfn = NNP_IPC_DMA_ADDR_TO_PFN(page_list);
+	rb_op_cmd = FIELD_PREP(NNP_H2C_OP_MASK, NNP_IPC_H2C_OP_CHANNEL_RB_OP);
+	rb_op_cmd |= FIELD_PREP(NNP_H2C_CHANNEL_RB_OP_CHAN_ID_MASK,
+				chan->chan_id);
+	rb_op_cmd |= FIELD_PREP(NNP_H2C_CHANNEL_RB_OP_ID_MASK, req.i_id);
+	rb_op_cmd |= FIELD_PREP(NNP_H2C_CHANNEL_RB_OP_HOST_PFN_MASK, dma_pfn);
+	if (req.i_h2c)
+		rb_op_cmd |= FIELD_PREP(NNP_H2C_CHANNEL_RB_OP_H2C_MASK, 1);
+
+	ret = send_rb_op(chan, rb_op_cmd, &req.o_errno);
+	if (ret || req.o_errno)
+		goto err_hostres_map;
+
+	ret = nnp_chan_set_ringbuf(chan, req.i_h2c, req.i_id, hostres);
+
+	if (ret == 0)
+		goto put_chan;
+
+err_hostres_map:
+	nnp_hostres_unmap_device(hostres, chan->nnpdev);
+put_chan:
+	nnp_chan_put(chan);
+do_exit:
+	if (copy_to_user(arg, &req, io_size) != 0)
+		return -EFAULT;
+
+	return ret;
+}
+
+static long destroy_channel_data_ringbuf(struct device_client_info *cinfo,
+					 void __user *arg, unsigned int size)
+{
+	struct nnp_device *nnpdev = cinfo->nnpdev;
+	struct ioctl_nnpi_destroy_channel_data_ringbuf req;
+	struct nnp_chan *chan;
+	u64 rb_op_cmd;
+	int ret;
+	unsigned int io_size = sizeof(req);
+
+	/* only single size structure is currently supported */
+	if (size != io_size)
+		return -EINVAL;
+
+	ret = copy_from_user(&req, arg, io_size);
+	if (ret != 0)
+		return -EFAULT;
+
+	/* we have one bit in ipc protocol for ringbuf id for each direction */
+	if (req.i_id > 1)
+		return -EINVAL;
+
+	/* o_errno must be cleared on entry */
+	if (req.o_errno)
+		return -EINVAL;
+
+	chan = nnpdev_find_channel(nnpdev, req.i_channel_id);
+	if (!chan) {
+		req.o_errno = NNPER_NO_SUCH_CHANNEL;
+		goto done;
+	}
+
+	rb_op_cmd = FIELD_PREP(NNP_H2C_OP_MASK, NNP_IPC_H2C_OP_CHANNEL_RB_OP);
+	rb_op_cmd |= FIELD_PREP(NNP_H2C_CHANNEL_RB_OP_CHAN_ID_MASK,
+				chan->chan_id);
+	rb_op_cmd |= FIELD_PREP(NNP_H2C_CHANNEL_RB_OP_ID_MASK, req.i_id);
+	rb_op_cmd |= FIELD_PREP(NNP_H2C_CHANNEL_RB_OP_DESTROY_MASK, 1);
+	if (req.i_h2c)
+		rb_op_cmd |= FIELD_PREP(NNP_H2C_CHANNEL_RB_OP_H2C_MASK, 1);
+
+	ret = send_rb_op(chan, rb_op_cmd, &req.o_errno);
+	if (ret || req.o_errno)
+		goto put_chan;
+
+	ret = nnp_chan_set_ringbuf(chan, req.i_h2c, req.i_id, NULL);
+
+put_chan:
+	nnp_chan_put(chan);
+done:
+	if (copy_to_user(arg, &req, io_size) != 0)
+		return -EFAULT;
+
+	return ret;
+}
+
+static long map_hostres(struct device_client_info *cinfo,
+			void __user *arg, unsigned int size)
+{
+	struct nnp_device *nnpdev = cinfo->nnpdev;
+	struct ioctl_nnpi_channel_map_hostres req;
+	struct nnp_chan *chan = NULL;
+	struct user_hostres *hostres_entry = NULL;
+	struct host_resource *hostres;
+	struct nnp_user_info *nnp_user = NULL;
+	struct chan_hostres_map *hostres_map = NULL;
+	u64 cmd[2];
+	unsigned long dma_pfn;
+	dma_addr_t page_list;
+	u32 total_chunks;
+	int map_id;
+	int ret = 0;
+	unsigned int io_size = sizeof(req);
+	const struct dma_map_ops *ops;
+
+	/* only single size structure is currently supported */
+	if (size != io_size)
+		return -EINVAL;
+
+	if (copy_from_user(&req, arg, io_size))
+		return -EFAULT;
+
+	if (req.i_hostres_handle >= INT_MAX)
+		return -EINVAL;
+
+	req.o_errno = 0;
+
+	chan = nnpdev_find_channel(nnpdev, req.i_channel_id);
+	if (!chan) {
+		req.o_errno = NNPER_NO_SUCH_CHANNEL;
+		goto do_exit;
+	}
+
+	nnp_user = chan->nnp_user;
+	mutex_lock(&nnp_user->mutex);
+	hostres_entry = idr_find(&nnp_user->idr,
+				 (int)req.i_hostres_handle);
+	if (!hostres_entry) {
+		req.o_errno = NNPER_NO_SUCH_RESOURCE;
+		mutex_unlock(&nnp_user->mutex);
+		goto put_chan;
+	}
+	hostres = hostres_entry->hostres;
+
+	hostres_map = kzalloc(sizeof(*hostres_map), GFP_KERNEL);
+	if (!hostres_map) {
+		req.o_errno = ENOMEM;
+		mutex_unlock(&nnp_user->mutex);
+		goto put_chan;
+	}
+
+	map_id = -1;
+	spin_lock(&chan->lock);
+	ret = ida_simple_get(&chan->hostres_map_ida, 0, U16_MAX, GFP_KERNEL);
+	if (ret < 0) {
+		req.o_errno = ENOMEM;
+		ret = 0;
+		spin_unlock(&chan->lock);
+		mutex_unlock(&nnp_user->mutex);
+		goto err_map;
+	}
+	map_id = ret;
+	spin_unlock(&chan->lock);
+
+	ret = nnp_hostres_map_device(hostres, nnpdev, false, &page_list,
+				     &total_chunks);
+	if (ret != 0) {
+		ret = -EFAULT;
+		mutex_unlock(&nnp_user->mutex);
+		goto err_ida;
+	}
+
+	/*
+	 * Its OK to release the mutex here and let other
+	 * thread destroy the hostres handle as we already
+	 * mapped it (which ref counted)
+	 */
+	mutex_unlock(&nnp_user->mutex);
+
+	dma_pfn = NNP_IPC_DMA_ADDR_TO_PFN(page_list);
+	cmd[0] = FIELD_PREP(NNP_H2C_OP_MASK, NNP_IPC_H2C_OP_CHANNEL_HOSTRES_OP);
+	cmd[0] |= FIELD_PREP(NNP_H2C_CHANNEL_HOSTRES_QW0_CHAN_ID_MASK,
+			     chan->chan_id);
+	cmd[0] |= FIELD_PREP(NNP_H2C_CHANNEL_HOSTRES_QW0_ID_MASK, map_id);
+	cmd[1] = FIELD_PREP(NNP_H2C_CHANNEL_HOSTRES_QW1_HOST_PFN_MASK, dma_pfn);
+
+	hostres_map->event_msg.value = 0;
+	hostres_map->id = (u16)map_id;
+	hostres_map->hostres = hostres;
+
+	spin_lock(&chan->lock);
+	hash_add(chan->hostres_hash,
+		 &hostres_map->hash_node,
+		 hostres_map->id);
+	spin_unlock(&chan->lock);
+
+	ret = -EPIPE;
+	if (!chan_drv_fatal(chan))
+		ret = nnpdev_queue_msg(chan->cmdq, cmd);
+	if (ret < 0) {
+		req.o_errno = NNPER_DEVICE_ERROR;
+		ret = 0;
+		goto err_hostres_map;
+	}
+
+	ret = wait_event_interruptible(nnpdev->waitq,
+				       hostres_map->event_msg.value != 0 ||
+				       chan_drv_fatal(chan));
+
+	if (hostres_map->event_msg.value == 0) {
+		if (ret < 0) {
+			ret = -EINTR;
+		} else {
+			req.o_errno = NNPER_DEVICE_ERROR;
+			ret = 0;
+		}
+		goto err_hostres_map;
+	} else if (hostres_map->event_msg.event_code ==
+		   NNP_IPC_CHANNEL_MAP_HOSTRES_FAILED) {
+		req.o_errno =
+		  event_val_to_nnp_error(hostres_map->event_msg.event_val);
+		ret = 0;
+		goto err_hostres_map;
+	} else if (ret) {
+		goto err_hostres_map;
+	}
+
+	ops = get_dma_ops(nnpdev->hw_dev->dev);
+	if (!ops)
+		req.o_sync_needed =
+			!dev_is_dma_coherent(nnpdev->hw_dev->dev);
+	else
+		req.o_sync_needed = (ops->sync_sg_for_cpu ? 1 : 0);
+
+	req.o_map_id = (u16)map_id;
+
+	goto put_chan;
+
+err_hostres_map:
+	nnp_chan_unmap_hostres(chan, (u16)map_id);
+err_ida:
+	spin_lock(&chan->lock);
+	ida_simple_remove(&chan->hostres_map_ida, map_id);
+	spin_unlock(&chan->lock);
+err_map:
+	kfree(hostres_map);
+put_chan:
+	nnp_chan_put(chan);
+do_exit:
+	if (copy_to_user(arg, &req, io_size))
+		ret = -EFAULT;
+
+	return ret;
+}
+
+static long unmap_hostres(struct device_client_info *cinfo, void __user *arg,
+			  unsigned int size)
+{
+	struct nnp_device *nnpdev = cinfo->nnpdev;
+	struct ioctl_nnpi_channel_unmap_hostres req;
+	struct nnp_chan *chan = NULL;
+	struct chan_hostres_map *hostres_map;
+	u64 cmd[2];
+	long ret;
+	unsigned int io_size = sizeof(req);
+
+	/* only single size structure is currently supported */
+	if (size != io_size)
+		return -EINVAL;
+
+	ret = copy_from_user(&req, arg, io_size);
+	if (ret != 0)
+		return -EFAULT;
+
+	/* o_errno must be cleared on entry */
+	if (req.o_errno)
+		return -EINVAL;
+
+	chan = nnpdev_find_channel(nnpdev, req.i_channel_id);
+	if (!chan) {
+		req.o_errno = NNPER_NO_SUCH_CHANNEL;
+		goto done;
+	}
+
+	hostres_map = nnp_chan_find_map(chan, req.i_map_id);
+	if (!hostres_map) {
+		req.o_errno = NNPER_NO_SUCH_HOSTRES_MAP;
+		goto put_chan;
+	}
+
+	cmd[0] = FIELD_PREP(NNP_H2C_OP_MASK, NNP_IPC_H2C_OP_CHANNEL_HOSTRES_OP);
+	cmd[0] |= FIELD_PREP(NNP_H2C_CHANNEL_HOSTRES_QW0_CHAN_ID_MASK,
+			     chan->chan_id);
+	cmd[0] |= FIELD_PREP(NNP_H2C_CHANNEL_HOSTRES_QW0_ID_MASK, req.i_map_id);
+	cmd[0] |= FIELD_PREP(NNP_H2C_CHANNEL_HOSTRES_QW0_UNMAP_MASK, 1);
+	cmd[1] = 0;
+
+	ret = nnpdev_queue_msg(chan->cmdq, cmd);
+
+put_chan:
+	nnp_chan_put(chan);
+done:
+	if (copy_to_user(arg, &req, io_size) != 0)
+		return -EFAULT;
+
+	return ret;
+}
+
 static long device_ioctl(struct file *f, unsigned int cmd, unsigned long arg)
 {
 	struct device_client_info *client = f->private_data;
@@ -212,6 +623,16 @@ static long device_ioctl(struct file *f, unsigned int cmd, unsigned long arg)
 	switch (ioc_nr) {
 	case _IOC_NR(IOCTL_NNPI_DEVICE_CREATE_CHANNEL):
 		return create_channel(client, (void __user *)arg, size);
+	case _IOC_NR(IOCTL_NNPI_DEVICE_CREATE_CHANNEL_RB):
+		return create_channel_data_ringbuf(client,
+						   (void __user *)arg, size);
+	case _IOC_NR(IOCTL_NNPI_DEVICE_DESTROY_CHANNEL_RB):
+		return destroy_channel_data_ringbuf(client,
+						    (void __user *)arg, size);
+	case _IOC_NR(IOCTL_NNPI_DEVICE_CHANNEL_MAP_HOSTRES):
+		return map_hostres(client, (void __user *)arg, size);
+	case _IOC_NR(IOCTL_NNPI_DEVICE_CHANNEL_UNMAP_HOSTRES):
+		return unmap_hostres(client, (void __user *)arg, size);
 	default:
 		pr_err_ratelimited("%s(%d): Unsupported device IOCTL 0x%x\n",
 				   current->comm, task_pid_nr(current), cmd);
diff --git a/include/uapi/misc/intel_nnpi.h b/include/uapi/misc/intel_nnpi.h
index 07bc50e..b866db1 100644
--- a/include/uapi/misc/intel_nnpi.h
+++ b/include/uapi/misc/intel_nnpi.h
@@ -150,6 +150,43 @@ struct nnpdrv_ioctl_destroy_hostres {
 	_IOWR('D', 0, struct ioctl_nnpi_create_channel)
 
 /**
+ * IOCTL_NNPI_DEVICE_CREATE_CHANNEL_RB:
+ *
+ * A request to create a data ring buffer for a command channel object.
+ * This is used to transfer data together with command to the device.
+ * A device command may include a data size fields which indicate how much data
+ * has pushed into that ring-buffer object.
+ */
+#define IOCTL_NNPI_DEVICE_CREATE_CHANNEL_RB   \
+	_IOWR('D', 1, struct ioctl_nnpi_create_channel_data_ringbuf)
+
+/**
+ * IOCTL_NNPI_DEVICE_DESTROY_CHANNEL_RB:
+ *
+ * A request to destoy a data ring buffer allocated for a command channel.
+ */
+#define IOCTL_NNPI_DEVICE_DESTROY_CHANNEL_RB  \
+	_IOWR('D', 2, struct ioctl_nnpi_destroy_channel_data_ringbuf)
+
+/**
+ * IOCTL_NNPI_DEVICE_CHANNEL_MAP_HOSTRES:
+ *
+ * A request to map a host resource to a command channel object.
+ * Device commands can include "map id" of this mapping for referencing
+ * a host resource.
+ */
+#define IOCTL_NNPI_DEVICE_CHANNEL_MAP_HOSTRES \
+	_IOWR('D', 3, struct ioctl_nnpi_channel_map_hostres)
+
+/**
+ * IOCTL_NNPI_DEVICE_CHANNEL_UNMAP_HOSTRES:
+ *
+ * A request to unmap a host resource previously mapped to a command channel.
+ */
+#define IOCTL_NNPI_DEVICE_CHANNEL_UNMAP_HOSTRES \
+	_IOWR('D', 4, struct ioctl_nnpi_channel_unmap_hostres)
+
+/**
  * struct ioctl_nnpi_create_channel - IOCTL_NNPI_DEVICE_CREATE_CHANNEL payload
  * @i_host_fd: opened file descriptor to /dev/nnpi_host
  * @i_min_id: minimum range for channel id allocation
@@ -177,6 +214,80 @@ struct ioctl_nnpi_create_channel {
 	__u16    o_channel_id;
 };
 
+/**
+ * struct ioctl_nnpi_create_channel_data_ringbuf
+ * @i_hostres_handle: handle of a host resource which will be used to hold
+ *         the ring-buffer content.
+ * @i_channel_id: command channel id.
+ * @i_id: id of the ring buffer object (can be 0 or 1).
+ * @i_h2c: non-zero if this ring-buffer is for command submission use,
+ *         otherwise it is for responses.
+ * @o_errno: On input, must be set to 0.
+ *           On output, 0 on success, one of the NNPERR_* error codes on error.
+ *
+ * this is the payload for IOCTL_NNPI_DEVICE_CREATE_CHANNEL_RB ioctl
+ */
+struct ioctl_nnpi_create_channel_data_ringbuf {
+	__u64 i_hostres_handle;
+	__u32 i_channel_id;
+	__u32 i_id;
+	__u32 i_h2c;
+	__u32 o_errno;
+};
+
+/**
+ * struct ioctl_nnpi_destroy_channel_data_ringbuf
+ * @i_channel_id: command channel id.
+ * @i_id: id of the ring buffer object (can be 0 or 1).
+ * @i_h2c: true if this ring-buffer is for command submission use,
+ *         otherwise it is for responses.
+ * @o_errno: On input, must be set to 0.
+ *           On output, 0 on success, one of the NNPERR_* error codes on error.
+ *
+ * this is the payload for IOCTL_NNPI_DEVICE_DESTROY_CHANNEL_RB ioctl
+ */
+struct ioctl_nnpi_destroy_channel_data_ringbuf {
+	__u32 i_channel_id;
+	__u32 i_id;
+	__u32 i_h2c;
+	__u32 o_errno;
+};
+
+/**
+ * struct ioctl_nnpi_channel_map_hostres
+ * @i_hostres_handle: handle of a host resource to be mapped
+ * @i_channel_id: command channel id.
+ * @o_map_id: returns unique id of the mapping
+ * @o_sync_needed: returns non-zero if LOCK/UNLOCK_HOST_RESOURCE ioctls
+ *            needs to be used before/after accessing the resource from cpu.
+ * @o_errno: On input, must be set to 0.
+ *           On output, 0 on success, one of the NNPERR_* error codes on error.
+ *
+ * this is the payload for IOCTL_NNPI_DEVICE_CHANNEL_MAP_HOSTRES ioctl
+ */
+struct ioctl_nnpi_channel_map_hostres {
+	__u64 i_hostres_handle;
+	__u32 i_channel_id;
+	__u32 o_map_id;
+	__u32 o_sync_needed;
+	__u32 o_errno;
+};
+
+/**
+ * ioctl_nnpi_channel_unmap_hostres
+ * @i_channel_id: command channel id.
+ * @i_map_id: mapping id
+ * @o_errno: On input, must be set to 0.
+ *           On output, 0 on success, one of the NNPERR_* error codes on error.
+ *
+ * This is the payload for IOCTL_NNPI_DEVICE_CHANNEL_UNMAP_HOSTRES ioctl
+ */
+struct ioctl_nnpi_channel_unmap_hostres {
+	__u32 i_channel_id;
+	__u32 i_map_id;
+	__u32 o_errno;
+};
+
 /****************************************************************
  * Error code values - errors returned in o_errno fields of
  * above structures.
-- 
1.8.3.1


From 0880542e549ecc12144cdf9b57d9ec7157040ab8 Mon Sep 17 00:00:00 2001
From: Guy Zadicario <guy.zadicario@intel.com>
Date: Mon, 30 Mar 2020 08:48:32 +0300
Subject: [PATCH 04/19] misc: nnpi: host resource implementation

This patch adds the "host resource" implementation,
The interface is well described in the header file hostres.h

The host resource is a memory object that can be mapped to dma address
space of one or more NNP-I devices as well as mapped to user space.

There are three interfaces to create three different types of such resources
nnpdrv_hostres_create - allocate memory pages for the new resource.
nnpdrv_hostres_dma_buf_create - create host resource attached to existing dma-buf object.
nnpdrv_hostres_create_usermem - create host resource mapped to user-space allocated memory.

There are interfaces to map/unmap the resource to both device access and to user space
as well as interfaces to lock/unlock the resource for CPU or device access.

Signed-off-by: Guy Zadicario <guy.zadicario@intel.com>
---
 drivers/misc/intel-nnpi/Makefile  |    3 +-
 drivers/misc/intel-nnpi/hostres.c | 1064 +++++++++++++++++++++++++++++
 drivers/misc/intel-nnpi/hostres.h |  259 +++++++
 drivers/misc/intel-nnpi/nnp_log.h |    1 +
 4 files changed, 1326 insertions(+), 1 deletion(-)
 create mode 100644 drivers/misc/intel-nnpi/hostres.c
 create mode 100644 drivers/misc/intel-nnpi/hostres.h

diff --git a/drivers/misc/intel-nnpi/Makefile b/drivers/misc/intel-nnpi/Makefile
index 407e55f67206..62e24319287e 100644
--- a/drivers/misc/intel-nnpi/Makefile
+++ b/drivers/misc/intel-nnpi/Makefile
@@ -6,6 +6,7 @@
 
 obj-m	:= intel_nnpidrv.o
 
-intel_nnpidrv-y := nnpdrv_main.o pcie.o device.o msg_scheduler.o
+intel_nnpidrv-y := nnpdrv_main.o pcie.o device.o msg_scheduler.o \
+		   hostres.o
 
 ccflags-y += -I$(src)/if_include
diff --git a/drivers/misc/intel-nnpi/hostres.c b/drivers/misc/intel-nnpi/hostres.c
new file mode 100644
index 000000000000..43183ad5a0b6
--- /dev/null
+++ b/drivers/misc/intel-nnpi/hostres.c
@@ -0,0 +1,1064 @@
+// SPDX-License-Identifier: GPL-2.0-or-later
+
+/********************************************
+ * Copyright (C) 2019-2020 Intel Corporation
+ ********************************************/
+
+#include "hostres.h"
+#include <linux/module.h>
+#include <linux/slab.h>
+#include <linux/err.h>
+#include <linux/sort.h>
+#include <linux/sched.h>
+#include <linux/jiffies.h>
+#include <linux/wait.h>
+#include <linux/atomic.h>
+#ifdef CONFIG_DMA_SHARED_BUFFER
+#include <linux/dma-buf.h>
+#endif
+#include <linux/pagemap.h>
+#include "ipc_protocol.h"
+#include "nnp_debug.h"
+#include "nnp_log.h"
+
+
+struct dma_list {
+	void             *vaddr;
+	dma_addr_t        dma_addr;
+};
+
+struct dev_mapping {
+	struct kref       ref;
+	struct nnpdrv_host_resource *res;
+	struct device    *dev;
+	enum dma_data_direction dir;
+	struct sg_table  *sgt;
+	struct dma_list  *list;
+	unsigned int      num;
+	unsigned int      entry_pages;
+	struct list_head  node;
+
+	struct dma_buf_attachment *dma_att;
+};
+
+struct nnpdrv_host_resource {
+	void             *magic;
+	struct kref       ref;
+	size_t            size;
+	struct list_head  devices;
+	spinlock_t        lock;
+#ifdef DEBUG
+	bool              destroyed;
+#endif
+	enum dma_data_direction dir;
+	wait_queue_head_t access_waitq;
+	// == 0 <=> unlocked; > 0 <=> locked for read; -1 <=> locked for write
+	int               readers;
+	bool              user_locked;
+
+#ifdef CONFIG_DMA_SHARED_BUFFER
+	bool external_buf;
+#endif
+	bool user_memory_buf;
+	uint16_t start_offset;
+
+	union {
+		struct {
+			struct page **pages;
+			unsigned int n_pages;
+		};
+#ifdef CONFIG_DMA_SHARED_BUFFER
+		struct {
+			struct dma_buf *buf;
+		};
+#endif
+	};
+};
+
+/* Static asserts */
+NNP_STATIC_ASSERT(NENTS_PER_PAGE >= 1,
+"There should be place for at least 1 DMA chunk addr in every DMA chain page");
+
+static int hostres_min_order = 7;
+module_param(hostres_min_order, int, 0600);
+
+static atomic64_t s_total_hostres_size;
+
+/* Destroys DMA page list of DMA addresses */
+static void destroy_dma_list(struct dev_mapping *m)
+{
+	unsigned int i;
+
+	for (i = 0; i < m->num; ++i) {
+		NNP_ASSERT(m->list[i].vaddr != NULL);
+		dma_free_coherent(m->dev, m->entry_pages * NNP_PAGE_SIZE,
+				  m->list[i].vaddr, m->list[i].dma_addr);
+	}
+	kfree(m->list);
+}
+
+/* Really destroys host resource, when all references to it were released */
+static void release_hostres(struct kref *kref)
+{
+	struct nnpdrv_host_resource *r = container_of(kref,
+						      struct nnpdrv_host_resource,
+						      ref);
+	unsigned int i, n, order;
+
+	NNP_ASSERT(list_empty(&r->devices));
+
+#ifdef CONFIG_DMA_SHARED_BUFFER
+	if (r->external_buf) {
+		dma_buf_put(r->buf);
+		kfree(r);
+		return;
+	}
+#endif
+
+	if (!r->user_memory_buf) {
+		for (i = 0; i < r->n_pages; i += n) {
+			NNP_ASSERT(r->pages[i] != NULL);
+			order = compound_order(r->pages[i]);
+			n = (1u << order);
+			__free_pages(r->pages[i], order);
+//TODO: Possible optimization: call to free_hot_page/free_cold_page
+		}
+		vfree(r->pages);
+	} else {
+		release_pages(r->pages, r->n_pages);
+		vfree(r->pages);
+	}
+
+	kfree(r);
+}
+
+void nnpdrv_hostres_get(struct nnpdrv_host_resource *res)
+{
+	int ret;
+
+	NNP_ASSERT(res != NULL);
+
+	ret = kref_get_unless_zero(&res->ref);
+	NNP_ASSERT(ret != 0);
+};
+
+bool nnpdrv_hostres_put(struct nnpdrv_host_resource *res)
+{
+	NNP_ASSERT(res != NULL);
+
+	return kref_put(&res->ref, release_hostres);
+}
+
+/* Really destroys mapping to device, when all references to it were released */
+static void release_mapping(struct kref *kref)
+{
+	struct dev_mapping *m = container_of(kref,
+					     struct dev_mapping,
+					     ref);
+
+	destroy_dma_list(m);
+
+#ifdef CONFIG_DMA_SHARED_BUFFER
+	if (m->res->external_buf) {
+		dma_buf_unmap_attachment(m->dma_att, m->sgt, m->res->dir);
+		dma_buf_detach(m->res->buf, m->dma_att);
+	} else
+#endif
+	{
+		dma_unmap_sg(m->dev, m->sgt->sgl, m->sgt->orig_nents, m->res->dir);
+		sg_free_table(m->sgt);
+		kfree(m->sgt);
+	}
+
+	spin_lock(&m->res->lock);
+	list_del(&m->node);
+	spin_unlock(&m->res->lock);
+
+	nnpdrv_hostres_put(m->res);
+
+	kfree(m);
+}
+
+/* Increase reference count to mapping to the specific device */
+static inline int mapping_get(struct dev_mapping *m)
+{
+	return kref_get_unless_zero(&m->ref);
+};
+
+/* Decrease reference count to mapping to the specific device
+ * and destroy it, if reference count is decreased to zero
+ */
+static inline int mapping_put(struct dev_mapping *m)
+{
+	return kref_put(&m->ref, release_mapping);
+}
+
+/* Compare callback function for sort, to compare 2 pages */
+static int cmp_pfn(const void *p1, const void *p2)
+{
+	NNP_ASSERT(page_to_pfn(*(const struct page **)p1) -
+		   page_to_pfn(*(const struct page **)p2) != 0);
+	return page_to_pfn(*(const struct page **)p1) -
+	       page_to_pfn(*(const struct page **)p2);
+}
+
+static struct nnpdrv_host_resource *alloc_hostres(size_t size, enum dma_data_direction dir)
+{
+	struct nnpdrv_host_resource *r;
+
+	r = kmalloc(sizeof(struct nnpdrv_host_resource), GFP_KERNEL);
+	if (unlikely(r == NULL))
+		return r;
+
+	r->magic = nnpdrv_hostres_create;
+#ifdef DEBUG
+	r->destroyed = false;
+#endif
+	kref_init(&r->ref);
+	spin_lock_init(&r->lock);
+	init_waitqueue_head(&r->access_waitq);
+	r->readers = 0;
+	r->dir = dir;
+	r->size = size;
+	r->user_locked = false;
+	r->start_offset = 0;
+	INIT_LIST_HEAD(&r->devices);
+
+	return r;
+}
+
+int nnpdrv_hostres_create(size_t size, enum dma_data_direction dir, struct nnpdrv_host_resource **out_resource)
+{
+	int err;
+	struct nnpdrv_host_resource *r;
+	unsigned int i, j, n;
+	unsigned int order;
+
+	if (unlikely(out_resource == NULL || size == 0 || dir == DMA_NONE))
+		return -EINVAL;
+
+	r = alloc_hostres(size, dir);
+	if (unlikely(r == NULL))
+		return -ENOMEM;
+
+	r->user_memory_buf = false;
+#ifdef CONFIG_DMA_SHARED_BUFFER
+	r->external_buf = false;
+#endif
+
+	// In this place actual pages are allocated of PAGE_SIZE and not NNP_PAGE_SIZE
+	// This list will be used for sg_alloc_table
+	r->n_pages = DIV_ROUND_UP(size, PAGE_SIZE);
+	r->pages = vmalloc(r->n_pages * sizeof(struct page *));
+	if (unlikely(r->pages == NULL)) {
+		nnp_log_err(CREATE_COMMAND_LOG, "Failed to vmalloc %zu bytes array\n", (r->n_pages*sizeof(struct page *)));
+		err = -ENOMEM;
+		goto free_res;
+	}
+
+	order = hostres_min_order;
+	n = (1u << order);
+
+	for (i = 0; i < r->n_pages; i += n) {
+		while (order > 0 && (r->n_pages - i < n)) {
+			order--;
+			n >>= 1;
+		}
+
+		do {
+			r->pages[i] = alloc_pages(GFP_KERNEL | __GFP_COMP, order);
+			if (r->pages[i] == NULL && order > 0) {
+				order--;
+				n >>= 1;
+			} else
+				break;
+		} while (1);
+
+		if (unlikely(r->pages[i] == NULL)) {
+			nnp_log_err(CREATE_COMMAND_LOG, "Failed to alloc page%u\n", i);
+			err = -ENOMEM;
+			goto free_pages;
+		}
+
+		for (j = 1; j < n; j++)
+			r->pages[i+j] = r->pages[i] + j;
+	}
+	// adjacent pages can be joined to 1 chunk
+	sort(r->pages, r->n_pages, sizeof(r->pages[0]), cmp_pfn, NULL);
+
+	atomic64_add(size, &s_total_hostres_size);
+
+	*out_resource = r;
+	return 0;
+
+free_pages:
+	for (i = 0; i < r->n_pages; i += n) {
+		if (r->pages[i] == NULL)
+			break;
+		order = compound_order(r->pages[i]);
+		n = (1u << order);
+		__free_pages(r->pages[i], order);
+//TODO: Possible optimization: call to free_hot_page/free_cold_page
+	}
+	vfree(r->pages);
+free_res:
+	kfree(r);
+	return err;
+}
+
+int nnpdrv_hostres_create_usermem(uint64_t                      user_ptr,
+				  size_t                        size,
+				  enum dma_data_direction       dir,
+				  struct nnpdrv_host_resource **out_resource)
+{
+	int err;
+	struct nnpdrv_host_resource *r;
+	unsigned int pinned_pages = 0;
+
+	if (unlikely(out_resource == NULL || size == 0 || dir == DMA_NONE))
+		return -EINVAL;
+
+	// restrict for 4 byte alignment - is this enough?
+	if ((user_ptr & 0x3) != 0)
+		return -EINVAL;
+
+	r = alloc_hostres(size, dir);
+	if (unlikely(r == NULL))
+		return -ENOMEM;
+
+	r->user_memory_buf = true;
+#ifdef CONFIG_DMA_SHARED_BUFFER
+	r->external_buf = false;
+#endif
+
+	r->start_offset = user_ptr & (PAGE_SIZE - 1);
+	user_ptr &= ~(PAGE_SIZE - 1);
+
+	// In this place actual pages are allocated of PAGE_SIZE and not NNP_PAGE_SIZE
+	// This list will be used for sg_alloc_table
+	r->n_pages = DIV_ROUND_UP(size + r->start_offset, PAGE_SIZE);
+	r->pages = vmalloc(r->n_pages * sizeof(struct page *));
+	if (unlikely(r->pages == NULL)) {
+		nnp_log_err(CREATE_COMMAND_LOG, "Failed to vmalloc %zu bytes array\n", (r->n_pages*sizeof(struct page *)));
+		err = -ENOMEM;
+		goto free_res;
+	}
+
+	do {
+		int n = get_user_pages(user_ptr + pinned_pages * PAGE_SIZE,
+				       r->n_pages - pinned_pages,
+				       (dir == DMA_TO_DEVICE || dir == DMA_BIDIRECTIONAL) ? FOLL_WRITE : 0,
+				       &r->pages[pinned_pages],
+				       NULL);
+		if (n < 0) {
+			err = -ENOMEM;
+			goto free_pages;
+		}
+
+		pinned_pages += n;
+	} while (pinned_pages < r->n_pages);
+
+	atomic64_add(size, &s_total_hostres_size);
+
+	*out_resource = r;
+	return 0;
+
+free_pages:
+	release_pages(r->pages, pinned_pages);
+	vfree(r->pages);
+free_res:
+	kfree(r);
+	return err;
+}
+
+int nnpdrv_hostres_dma_buf_create(int dma_buf_fd, enum dma_data_direction dir, struct nnpdrv_host_resource **out_resource)
+{
+#ifdef CONFIG_DMA_SHARED_BUFFER
+	int err;
+	struct nnpdrv_host_resource *r;
+	struct dma_buf *dmabuf;
+
+	if (unlikely(out_resource == NULL || dma_buf_fd < 0 || dir == DMA_NONE))
+		return -EINVAL;
+
+	dmabuf = dma_buf_get(dma_buf_fd);
+	err = PTR_ERR_OR_ZERO(dmabuf);
+	if (unlikely(err < 0))
+		//EBADF in case of dma_buf_fd is not fd;
+		//EINVAL in case dma_buf_fd is fd, but not of dma_buf
+		//in any case report invalid value
+		return -EINVAL;
+
+	r = alloc_hostres(dmabuf->size, dir);
+	if (unlikely(r == NULL)) {
+		dma_buf_put(dmabuf);
+		return -ENOMEM;
+	}
+
+	r->buf = dmabuf;
+	r->user_memory_buf = false;
+	r->external_buf = true;
+
+	*out_resource = r;
+	return 0;
+#else
+	return -EBADF;
+#endif
+}
+
+int nnpdrv_hostres_vmap(struct nnpdrv_host_resource *res,
+			void                       **out_ptr)
+{
+	if (!res || !out_ptr)
+		return -EINVAL;
+
+	/*
+	 * dma-buf case
+	 */
+#ifdef CONFIG_DMA_SHARED_BUFFER
+	if (res->external_buf) {
+		*out_ptr = dma_buf_vmap(res->buf);
+		if (*out_ptr)
+			return 0;
+
+		return -ENOMEM;
+	}
+#endif
+
+	/*
+	 * no dma-buf case
+	 */
+	*out_ptr = vmap(res->pages, res->n_pages, 0, PAGE_KERNEL);
+	if (*out_ptr)
+		return 0;
+
+	return -ENOMEM;
+}
+
+void nnpdrv_hostres_vunmap(struct nnpdrv_host_resource *res, void *ptr)
+{
+	/*
+	 * dma-buf case
+	 */
+#ifdef CONFIG_DMA_SHARED_BUFFER
+	if (res->external_buf) {
+		dma_buf_vunmap(res->buf, ptr);
+		return;
+	}
+#endif
+
+	/*
+	 * no dma-buf case
+	 */
+	vunmap(ptr);
+}
+
+int nnpdrv_hostres_read_refcount(struct nnpdrv_host_resource *res)
+{
+	return kref_read(&res->ref);
+}
+
+bool nnpdrv_hostres_destroy(struct nnpdrv_host_resource *res)
+{
+	NNP_ASSERT(res != NULL);
+
+#ifdef DEBUG
+	spin_lock(&res->lock);
+	NNP_ASSERT(!res->destroyed);
+	res->destroyed = true;
+	spin_unlock(&res->lock);
+#endif
+
+#ifdef CONFIG_DMA_SHARED_BUFFER
+	if (!res->external_buf)
+#endif
+		atomic64_sub(res->size, &s_total_hostres_size);
+
+	return nnpdrv_hostres_put(res);
+}
+
+/* Finds mapping by device. NULL if not found*/
+static struct dev_mapping *mapping_for_dev(struct nnpdrv_host_resource *res, struct device *dev)
+{
+	struct dev_mapping *m;
+
+	spin_lock(&res->lock);
+
+	list_for_each_entry(m, &res->devices, node) {
+		if (m->dev == dev)
+			break;
+	}
+
+	spin_unlock(&res->lock);
+
+	// Check if no mapping found
+	if (&m->node == &res->devices)
+		return NULL;
+
+	return m;
+}
+
+/* Builds DMA page list with DMA addresses of the mapped host resource */
+static int build_dma_list(struct dev_mapping *m, bool use_one_entry, uint32_t start_offset)
+{
+	unsigned int i, k = 0;
+	int err;
+#ifdef DEBUG
+	unsigned int j = 0;
+#endif
+	struct dma_chain_entry *p = NULL;
+	struct dma_chain_header *h;
+	struct scatterlist *sg;
+	unsigned int nents_per_entry;
+
+	if (unlikely(use_one_entry)) {
+		nents_per_entry = m->sgt->nents;
+		m->entry_pages = DIV_ROUND_UP(sizeof(struct dma_chain_header) +
+				m->sgt->nents * sizeof(struct dma_chain_entry),
+				NNP_PAGE_SIZE);
+		m->num = 1;
+	} else {
+		nents_per_entry = NENTS_PER_PAGE;
+		m->entry_pages = 1;
+		m->num = DIV_ROUND_UP(m->sgt->nents, NENTS_PER_PAGE);
+	}
+
+	m->list = kmalloc_array(m->num, sizeof(struct dma_list), GFP_KERNEL);
+	if (unlikely(m->list == NULL))
+		return -ENOMEM;
+
+	for (i = 0; i < m->num; ++i) {
+		m->list[i].vaddr = dma_alloc_coherent(m->dev,
+						      m->entry_pages * NNP_PAGE_SIZE,
+						      &m->list[i].dma_addr,
+						      GFP_KERNEL);
+		if (unlikely(m->list[i].vaddr == NULL)) {
+			err = -ENOMEM;
+			goto free_pages;
+		}
+		// Check that 4K aligned dma_addr can fit 45 bit pfn
+		NNP_ASSERT(NNP_IPC_DMA_PFN_TO_ADDR(NNP_IPC_DMA_ADDR_TO_PFN(m->list[i].dma_addr)) == m->list[i].dma_addr);
+	}
+
+	// join links in the chain
+	for (i = 0; i < m->num - 1; ++i) {
+		h = (struct dma_chain_header *)m->list[i].vaddr;
+		h->dma_next = m->list[i + 1].dma_addr;
+		h->total_nents = m->sgt->nents;
+		h->start_offset = (i == 0 ? start_offset : 0);
+	}
+	h = (struct dma_chain_header *)m->list[i].vaddr;
+	h->dma_next = 0;
+	h->total_nents = m->sgt->nents;
+	h->start_offset = (i == 0 ? start_offset : 0);
+
+	sg = m->sgt->sgl;
+	NNP_ASSERT(m->num > 0);
+	for (i = 0; i < m->num; ++i) {
+		h = (struct dma_chain_header *)m->list[i].vaddr;
+		h->size = 0;
+		p = (struct dma_chain_entry *)(m->list[i].vaddr + sizeof(struct dma_chain_header));
+		for (k = 0; k < nents_per_entry && !sg_is_last(sg); ++k) {
+#ifdef DEBUG
+			NNP_ASSERT(sg != NULL);
+			NNP_ASSERT(j < m->sgt->nents - 1);
+			++j;
+#endif
+			p[k].dma_chunk_pfn = NNP_IPC_DMA_ADDR_TO_PFN(sg_dma_address(sg));
+			// Check that 4K aligned dma_addr can fit 45 bit pfn
+			NNP_ASSERT(NNP_IPC_DMA_PFN_TO_ADDR(p[k].dma_chunk_pfn) == sg_dma_address(sg));
+			//chunk size in pages
+			p[k].n_pages = sg_dma_len(sg) / NNP_PAGE_SIZE;
+			// Assert that sg_dma_len is aligned to NNP_PAGE_SIZE
+			NNP_ASSERT(sg_dma_len(sg) % NNP_PAGE_SIZE == 0);
+			// Assert that chunk size fits in 19 bits
+			NNP_ASSERT(p[k].n_pages * NNP_PAGE_SIZE == sg_dma_len(sg));
+
+			h->size += sg_dma_len(sg);
+
+			sg = sg_next(sg);
+		}
+	}
+
+	NNP_ASSERT(sg_is_last(sg));
+	p[k].dma_chunk_pfn = NNP_IPC_DMA_ADDR_TO_PFN(sg_dma_address(sg));
+	// last chunk size in pages rounded up
+	p[k].n_pages = DIV_ROUND_UP(sg_dma_len(sg), NNP_PAGE_SIZE);
+	NNP_ASSERT(p[k].n_pages * NNP_PAGE_SIZE >= sg_dma_len(sg));
+	NNP_ASSERT(p[k].n_pages * NNP_PAGE_SIZE - sg_dma_len(sg) <= NNP_PAGE_SIZE);
+
+	h->size += sg_dma_len(sg);
+
+#ifdef DEBUG
+	++k;
+	// SECURITY ??
+	memset(p + k, 0xcc, m->entry_pages*NNP_PAGE_SIZE - sizeof(struct dma_chain_header) - sizeof(struct dma_chain_entry) * k);
+#endif
+
+	return 0;
+
+free_pages:
+	for (i = 0; i < m->num; ++i) {
+		if (m->list[i].vaddr == NULL)
+			break;
+		dma_free_coherent(m->dev, m->entry_pages * NNP_PAGE_SIZE, m->list[i].vaddr, m->list[i].dma_addr);
+	}
+	kfree(m->list);
+
+	return err;
+}
+
+int nnpdrv_hostres_map_device(struct nnpdrv_host_resource *res,
+			      struct nnp_device           *nnpdev,
+			      bool                         use_one_entry,
+			      dma_addr_t                  *page_list,
+			      uint32_t                    *total_chunks)
+{
+	int ret;
+	struct dev_mapping *m;
+
+	if (unlikely(res == NULL || nnpdev == NULL || page_list == NULL))
+		return -EINVAL;
+
+	//Check if already mapped for the device
+	m = mapping_for_dev(res, nnpdev->hw_device_info->hw_device);
+	// "mapping_get" will fail and return 0, if m is at destroy stage
+	// so if mapping is exist and it is not being destroyed,
+	// "mapping_get" will successfully increase ref and will return 1
+	if (likely(m != NULL && mapping_get(m) == 1)) {
+		*page_list = m->list[0].dma_addr;
+		return 0;
+	}
+
+	nnpdrv_hostres_get(res);
+
+	m = kmalloc(sizeof(struct dev_mapping), GFP_KERNEL);
+	if (unlikely(m == NULL)) {
+		ret = -ENOMEM;
+		goto put_resource;
+	}
+
+	kref_init(&m->ref);
+
+	m->dev = nnpdev->hw_device_info->hw_device;
+	m->res = res;
+
+#ifdef CONFIG_DMA_SHARED_BUFFER
+	if (res->external_buf) {
+		m->dma_att = dma_buf_attach(res->buf, m->dev);
+		ret = PTR_ERR_OR_ZERO(m->dma_att);
+		if (unlikely(ret < 0))
+			goto free_mapping;
+
+		m->sgt = dma_buf_map_attachment(m->dma_att, res->dir);
+		ret = PTR_ERR_OR_ZERO(m->sgt);
+		if (unlikely(ret < 0))
+			goto buf_detach;
+	} else
+#endif
+	{
+		m->sgt = kmalloc(sizeof(struct sg_table), GFP_KERNEL);
+		if (unlikely(m->sgt == NULL)) {
+			ret = -ENOMEM;
+			goto free_mapping;
+		}
+
+		ret = __sg_alloc_table_from_pages(m->sgt,
+						res->pages,
+						res->n_pages,
+						0,
+						res->size + res->start_offset,
+						NNP_MAX_CHUNK_SIZE,
+						GFP_KERNEL);
+		if (unlikely(ret < 0))
+			goto free_sgt_struct;
+
+		ret = dma_map_sg(m->dev, m->sgt->sgl, m->sgt->orig_nents, res->dir);
+		if (unlikely(ret < 0))
+			goto free_sgt;
+
+		m->sgt->nents = ret;
+	}
+
+	ret = build_dma_list(m, use_one_entry, res->start_offset);
+	if (unlikely(ret < 0))
+		goto unmap;
+
+	spin_lock(&res->lock);
+	list_add(&m->node, &res->devices);
+	spin_unlock(&res->lock);
+
+	*page_list = m->list[0].dma_addr;
+	if (total_chunks)
+		*total_chunks = m->sgt->nents;
+
+	return 0;
+
+unmap:
+#ifdef CONFIG_DMA_SHARED_BUFFER
+	if (res->external_buf) {
+		dma_buf_unmap_attachment(m->dma_att, m->sgt, res->dir);
+buf_detach:
+		dma_buf_detach(res->buf, m->dma_att);
+	} else
+#endif
+	{
+		dma_unmap_sg(m->dev, m->sgt->sgl, m->sgt->orig_nents, res->dir);
+free_sgt:
+		sg_free_table(m->sgt);
+free_sgt_struct:
+		kfree(m->sgt);
+	}
+free_mapping:
+	kfree(m);
+put_resource:
+	nnpdrv_hostres_put(res);
+	return ret;
+}
+
+int nnpdrv_hostres_unmap_device(struct nnpdrv_host_resource *res, struct nnp_device *nnpdev)
+{
+	struct dev_mapping *m;
+
+	if (unlikely(res == NULL))
+		return -EINVAL;
+
+	m = mapping_for_dev(res, nnpdev->hw_device_info->hw_device);
+	if (unlikely(m == NULL))
+		return -ENXIO;
+
+	mapping_put(m);
+
+	return 0;
+}
+
+int nnpdrv_hostres_map_user(struct nnpdrv_host_resource *res, struct vm_area_struct *vma)
+{
+	unsigned int i, j, offset = 0, seg_len, s;
+	int err;
+
+	if (unlikely(res == NULL || vma == NULL))
+		return -EINVAL;
+
+#ifdef CONFIG_DMA_SHARED_BUFFER
+	if (res->external_buf)
+		return -EPERM;
+#endif
+
+	if (res->user_memory_buf)
+		return -EINVAL;
+
+	if (unlikely(vma->vm_end - vma->vm_start < res->size))
+		return -EINVAL;
+
+	s = res->size;
+	for (i = 0; i < res->n_pages - 1; i += j) {
+		seg_len = 0;
+		for (j = 0; i + j < res->n_pages - 1; ++j) {
+			seg_len += NNP_PAGE_SIZE;
+			if (page_to_pfn(res->pages[i + j]) != page_to_pfn(res->pages[i + j + 1]) + 1) {
+				err = remap_pfn_range(vma, vma->vm_start + offset,
+						      page_to_pfn(res->pages[i]), seg_len, vma->vm_page_prot);
+				if (unlikely(err < 0))
+					return err;
+				++j;
+				s -= seg_len;
+				offset += seg_len;
+				break;
+			}
+		}
+	}
+	err = remap_pfn_range(vma, vma->vm_start + offset, page_to_pfn(res->pages[i]), s, vma->vm_page_prot);
+
+	return err;
+}
+
+int nnpdrv_hostres_dev_lock(struct nnpdrv_host_resource *res, struct nnp_device *nnpdev, enum dma_data_direction dir)
+{
+	if (unlikely(res == NULL)) {
+		nnp_log_err(GENERAL_LOG, "Host resource is null\n");
+		return -EINVAL;
+	}
+	if (unlikely(dir == DMA_NONE)) {
+		nnp_log_err(GENERAL_LOG, "Host resource must have a direction\n");
+		return -EINVAL;
+	}
+
+	if (unlikely(res->dir != DMA_BIDIRECTIONAL && dir != res->dir)) {
+		nnp_log_err(GENERAL_LOG, "Host resource direction (%u) must fit lock direction (%u)\n", res->dir, dir);
+		return -EPERM;
+	}
+
+	NNP_ASSERT(mapping_for_dev(res, nnpdev->hw_device_info->hw_device) != NULL);
+
+	spin_lock(&res->lock);
+	// Check if requested access is Read Only
+	if (dir == DMA_TO_DEVICE) {
+		if (unlikely(res->readers < 0)) {
+			nnp_log_err(GENERAL_LOG, "Error: lock is busy for read\n");
+			NNP_ASSERT(res->readers == -1);
+			spin_unlock(&res->lock);
+			return -EBUSY;
+		}
+		++res->readers;
+	} else {
+		if (unlikely(res->readers != 0)) {
+			nnp_log_err(GENERAL_LOG, "Error: lock is busy for write\n");
+			spin_unlock(&res->lock);
+			return -EBUSY;
+		}
+		res->readers = -1;
+	}
+	spin_unlock(&res->lock);
+
+	return 0;
+}
+
+int nnpdrv_hostres_dev_unlock(struct nnpdrv_host_resource *res, struct nnp_device *nnpdev, enum dma_data_direction dir)
+{
+	if (unlikely(res == NULL || nnpdev == NULL || dir == DMA_NONE))
+		return -EINVAL;
+
+	if (unlikely(res->dir != DMA_BIDIRECTIONAL && dir != res->dir))
+		return -EPERM;
+
+	NNP_ASSERT(mapping_for_dev(res, nnpdev->hw_device_info->hw_device) != NULL);
+
+	spin_lock(&res->lock);
+	// Check if requested access is Read Only
+	if (dir == DMA_TO_DEVICE) {
+		NNP_ASSERT(res->readers > 0);
+		--res->readers;
+	} else {
+		NNP_ASSERT(res->readers == -1);
+		res->readers = 0;
+	}
+	spin_unlock(&res->lock);
+
+	wake_up_all(&res->access_waitq);
+
+	return 0;
+}
+
+#define GET_WAIT_EVENT_ERR(ret)					\
+	((ret) > 0 ?						\
+		(0) /* wait completed before timeout elapsed */	\
+	:							\
+		((ret) == 0 ?					\
+			(-ETIME) /* timed out */		\
+		:						\
+			/* ERESTARTSYS should not be returned to user. Convert it to EINTR. */\
+			((ret) == -ERESTARTSYS ?		\
+				(-EINTR)			\
+			:					\
+				(ret)				\
+			)					\
+		)						\
+	)
+
+int nnpdrv_hostres_user_lock(struct nnpdrv_host_resource *res, unsigned int timeout)
+{
+	long ret, left, before;
+
+	left = (timeout != U32_MAX ? usecs_to_jiffies(timeout) : MAX_SCHEDULE_TIMEOUT);
+
+	if (unlikely(res == NULL || (unsigned long)left > MAX_SCHEDULE_TIMEOUT))
+		return -EINVAL;
+
+	// No need to get kref, as it should be already got
+	spin_lock(&res->lock);
+	if (res->user_locked) {
+		spin_unlock(&res->lock);
+		return -EINVAL;
+	}
+	spin_unlock(&res->lock);
+
+	spin_lock(&res->lock);
+	// Check if requested access is Read Only
+	if (res->dir == DMA_FROM_DEVICE) {
+		while (res->readers < 0 && left > 0) {
+			NNP_ASSERT(res->readers == -1);
+
+			spin_unlock(&res->lock);
+
+			before = jiffies;
+			ret = wait_event_interruptible_timeout(res->access_waitq,
+							       (res->readers >= 0),
+							       left);
+			ret = GET_WAIT_EVENT_ERR(ret);
+			if (unlikely(ret < 0))
+				return ret;
+
+			spin_lock(&res->lock);
+			left -= jiffies - before;
+		}
+
+		if (likely(res->readers >= 0)) {
+			++res->readers;
+			ret = 0;
+		} else {
+			ret = -ETIME;
+		}
+	} else {
+		while (res->readers != 0 && left > 0) {
+			spin_unlock(&res->lock);
+
+			before = jiffies;
+			ret = wait_event_interruptible_timeout(res->access_waitq,
+							       (res->readers == 0),
+							       left);
+			ret = GET_WAIT_EVENT_ERR(ret);
+			if (unlikely(ret < 0))
+				return ret;
+
+			spin_lock(&res->lock);
+			left -= jiffies - before;
+		}
+
+		if (likely(res->readers == 0)) {
+			res->readers = -1;
+			ret = 0;
+		} else {
+			ret = -ETIME;
+		}
+	}
+
+	if (ret == -ETIME && timeout == 0)
+		ret = -EBUSY;
+
+
+	if (unlikely(ret < 0)) {
+		spin_unlock(&res->lock);
+		return ret;
+	}
+
+	res->user_locked = true;
+
+#ifdef CONFIG_DMA_SHARED_BUFFER
+	if (res->external_buf) {
+		spin_unlock(&res->lock);
+		ret = dma_buf_begin_cpu_access(res->buf, res->dir);
+	} else
+#endif
+	{
+		struct dev_mapping *m;
+
+		list_for_each_entry(m, &res->devices, node)
+			dma_sync_sg_for_cpu(m->dev, m->sgt->sgl, m->sgt->orig_nents, res->dir);
+
+		spin_unlock(&res->lock);
+	}
+
+	return ret;
+}
+
+int nnpdrv_hostres_user_unlock(struct nnpdrv_host_resource *res)
+{
+	if (unlikely(res == NULL))
+		return -EINVAL;
+
+	// No need to get kref, as it should be already got
+	spin_lock(&res->lock);
+	if (!res->user_locked) {
+		spin_unlock(&res->lock);
+		return -EINVAL;
+	}
+	spin_unlock(&res->lock);
+
+	spin_lock(&res->lock);
+	// Check if requested access is Read Only
+	if (res->dir == DMA_FROM_DEVICE) {
+		NNP_ASSERT(res->readers > 0);
+		--res->readers;
+	} else {
+		NNP_ASSERT(res->readers == -1);
+		res->readers = 0;
+	}
+
+	res->user_locked = false;
+
+#ifdef CONFIG_DMA_SHARED_BUFFER
+	if (res->external_buf) {
+		spin_unlock(&res->lock);
+		dma_buf_end_cpu_access(res->buf, res->dir);
+	} else
+#endif
+	{
+		struct dev_mapping *m;
+
+		list_for_each_entry(m, &res->devices, node)
+			dma_sync_sg_for_device(m->dev, m->sgt->sgl, m->sgt->orig_nents, res->dir);
+
+		spin_unlock(&res->lock);
+	}
+
+	wake_up_all(&res->access_waitq);
+
+	return 0;
+}
+
+bool nnpdrv_hostres_is_input(struct nnpdrv_host_resource *res)
+{
+	NNP_ASSERT(res != NULL);
+
+	return (res->dir == DMA_TO_DEVICE || res->dir == DMA_BIDIRECTIONAL);
+}
+
+bool nnpdrv_hostres_is_output(struct nnpdrv_host_resource *res)
+{
+	NNP_ASSERT(res != NULL);
+
+	return (res->dir == DMA_FROM_DEVICE || res->dir == DMA_BIDIRECTIONAL);
+}
+
+size_t nnpdrv_hostres_get_size(struct nnpdrv_host_resource *res)
+{
+	if (unlikely(res == NULL))
+		return 0;
+
+	return res->size;
+}
+
+bool nnpdrv_hostres_is_usermem(struct nnpdrv_host_resource *res)
+{
+	NNP_ASSERT(res != NULL);
+
+	return res->user_memory_buf;
+}
+
+static ssize_t total_hostres_size_show(struct device           *dev,
+				       struct device_attribute *attr,
+				       char                    *buf)
+{
+	ssize_t ret = 0;
+
+	ret += snprintf(&buf[ret], PAGE_SIZE - ret, "%llu\n", (u64)atomic64_read(&s_total_hostres_size));
+
+	return ret;
+}
+static DEVICE_ATTR_RO(total_hostres_size);
+
+static struct attribute *nnp_host_attrs[] = {
+	&dev_attr_total_hostres_size.attr,
+	NULL
+};
+
+static struct attribute_group nnp_host_attrs_grp = {
+		.attrs = nnp_host_attrs
+};
+
+int nnpdrv_hostres_init_sysfs(struct kobject *kobj)
+{
+	int ret;
+
+	atomic64_set(&s_total_hostres_size, 0);
+
+	ret = sysfs_create_group(kobj, &nnp_host_attrs_grp);
+
+	return ret;
+}
+
+void nnpdrv_hostres_fini_sysfs(struct kobject *kobj)
+{
+	sysfs_remove_group(kobj, &nnp_host_attrs_grp);
+}
diff --git a/drivers/misc/intel-nnpi/hostres.h b/drivers/misc/intel-nnpi/hostres.h
new file mode 100644
index 000000000000..33b0d7cdacbe
--- /dev/null
+++ b/drivers/misc/intel-nnpi/hostres.h
@@ -0,0 +1,259 @@
+/* SPDX-License-Identifier: GPL-2.0-or-later */
+
+/********************************************
+ * Copyright (C) 2019-2020 Intel Corporation
+ ********************************************/
+
+#ifndef _NNPDRV_HOSTRES_H
+#define _NNPDRV_HOSTRES_H
+
+#include <linux/dma-mapping.h>
+#include "device.h"
+
+struct nnpdrv_host_resource;
+
+
+/**
+ * @brief Creates host resource
+ *
+ * This function provides host resource handle. The resource can be
+ * Input(read by device), Output(write by device) and both.
+ * If this function fails, it frees all already allocated resources
+ * and exits with error. So inconsistent state is eliminated.
+ * In case of failure destroy function should not be called.
+ *
+ * @param[in]   size  Size of the host resource to be created
+ * @param[in]   dir   Resource direction (read or write or both)
+ * @param[out]  res   Handle to newly created hosr resource
+ * @return error number on failure.
+ */
+int nnpdrv_hostres_create(size_t                        size,
+			  enum                          dma_data_direction dir,
+			  struct nnpdrv_host_resource **res);
+
+/**
+ * @brief Creates host resource
+ *
+ * This function provides host resource handle. The resource can be
+ * Input(read by device), Output(write by device) and both.
+ * If this function fails, it frees all already allocated resources
+ * and exits with error. So inconsistent state is eliminated.
+ * In case of failure destroy function should not be called.
+ *
+ * @param[in]   dma_buf_fd  File descriptor of struct dma_buf
+ * @param[in]   dir         Resource direction (read or write or both)
+ * @param[out]  res         Handle to newly created hosr resource
+ * @return error number on failure.
+ */
+int nnpdrv_hostres_dma_buf_create(int                           dma_buf_fd,
+				  enum dma_data_direction       dir,
+				  struct nnpdrv_host_resource **res);
+
+/**
+ * @brief Creates host resource from user allocated memory
+ *
+ * This function provides host resource handle. The resource can be
+ * Input(read by device), Output(write by device) and both.
+ * If this function fails, it frees all already allocated resources
+ * and exits with error. So inconsistent state is eliminated.
+ * In case of failure destroy function should not be called.
+ *
+ * @param[in]   dma_buf_fd  File descriptor of struct dma_buf
+ * @param[in]   dir         Resource direction (read or write or both)
+ * @param[out]  res         Handle to newly created hosr resource
+ * @return error number on failure.
+ */
+int nnpdrv_hostres_create_usermem(uint64_t                      user_ptr,
+				  size_t                        size,
+				  enum dma_data_direction       dir,
+				  struct nnpdrv_host_resource **out_resource);
+
+int nnpdrv_hostres_vmap(struct nnpdrv_host_resource *res,
+			void                       **out_ptr);
+
+void nnpdrv_hostres_vunmap(struct nnpdrv_host_resource *res, void *ptr);
+
+/**
+ * @brief Returns the refcount of the host resource
+ *
+ * This function returns the number of objects reference this
+ * host resource. After creation the refcount is 1.
+ *
+ * @param[in]  res  handle to the res
+ * @return num of reference of hostres
+ */
+int nnpdrv_hostres_read_refcount(struct nnpdrv_host_resource *res);
+
+/**
+ * @brief Destroys the host resource previously created
+ *
+ * This function releases all the resourses allocated for the host resource.
+ *
+ * @param[in]  res  handle to the res
+ * @return false if refcount is not 0
+ */
+bool nnpdrv_hostres_destroy(struct nnpdrv_host_resource *res);
+
+/**
+ * @brief Maps the host resource to SpringHill device
+ *
+ * This function maps the host resource to be accessible from device
+ * and returns the dma page list of DMA addresses.
+ * The resource can be mapped to multiple devices.
+ * The resource can be mapped to userspace and to device at the same time.
+ *
+ * @param[in]   res           handle to the host resource
+ * @param[in]   nnpdev        handle to the device
+ * @param[in]   use_one_entry use page list with one big enough entry
+ * @param[out]  page_list     DMA address of first DMA page from the page list
+ * @param[out]  total_chunks  Total number of DMA chunks in the all page list,
+ *                            May be NULL if not required.
+ * @return error on failure.
+ */
+int nnpdrv_hostres_map_device(struct nnpdrv_host_resource *res,
+			      struct nnp_device           *nnpdev,
+			      bool                         use_one_entry,
+			      dma_addr_t                  *page_list,
+			      uint32_t                    *total_chunks);
+
+/**
+ * @brief Unmaps the host resource from SpringHill device
+ *
+ * This function unmaps previously mapped host resource from device.
+ * The resource must be mapped to this device before calling this function.
+ * The resource must be unlocked from this device, if it was previously locked,
+ * before calling this function.
+ *
+ * @param[in]   res     handle to the host resource
+ * @param[in]   nnpdev  handle to the device
+ * @return error on failure.
+ */
+int nnpdrv_hostres_unmap_device(struct nnpdrv_host_resource *res,
+				struct nnp_device           *nnpdev);
+
+/**
+ * @brief Maps the host resource to userspace
+ *
+ * This function maps the host resource to userspace virtual memory.
+ * The host resource can be mapped to userspace multiple times.
+ * The host resource can be mapped to user and to device at the same time.
+ *
+ * @param[in]       res  handle to the host resource
+ * @param[in/out]   vma  handle to the virtual memory area
+ * @return error on failure.
+ */
+int nnpdrv_hostres_map_user(struct nnpdrv_host_resource *res,
+			    struct vm_area_struct       *vma);
+
+/**
+ * @brief Lock the host resource to for access from specified device
+ *
+ * This function locks the host resource from being modified by anyone else,
+ * neither by user, nor by any of other devices. So it can be safely
+ * read/modified from the device.
+ * The resource must be mapped to this device before calling this function.
+ *
+ * @param[in]  res     handle to the host resource
+ * @param[in]  nnpdev  handle to the device
+ * @param[in]  dir     desired access direction (read or write or both)
+ * @return error on failure.
+ */
+int nnpdrv_hostres_dev_lock(struct nnpdrv_host_resource *res,
+			    struct nnp_device           *nnpdev,
+			    enum dma_data_direction      dir);
+
+/**
+ * @brief Unlocks the host resource from being accessed by specified device
+ *
+ * This function unlocks previously locked host resource from the device.
+ *
+ * @param[in]  res     handle to the host resource
+ * @param[in]  nnpdev  handle to the device
+ * @param[in]  dir     desired access direction (read or write or both)
+ * @return error on failure.
+ */
+int nnpdrv_hostres_dev_unlock(struct nnpdrv_host_resource *res,
+			      struct nnp_device           *nnpdev,
+			      enum dma_data_direction      dir);
+
+/**
+ * @brief Lock the host resource to for access from userspace
+ *
+ * This function locks the host resource from being modified by any of devices.
+ * So it can be safely read or modified from user space.
+ * The resource must be mapped to userspace before calling this function.
+ *
+ * @param[in]  res      handle to the host resource
+ * @param[in]  timeout  timeout in usec.
+ *                      0 -- don't wait; "too big" timeout -- wait forever
+ * @return error on failure.
+ */
+int nnpdrv_hostres_user_lock(struct nnpdrv_host_resource *res,
+			     unsigned int                 timeout);
+
+/**
+ * @brief Unlocks the host resource from being accessed by userspace
+ *
+ * This function unlocks previously locked host resource from userspace.
+ *
+ * @param[in]  res  handle to the host resource
+ * @return error on failure.
+ */
+int nnpdrv_hostres_user_unlock(struct nnpdrv_host_resource *res);
+
+/**
+ * @brief Increases refcount of the hostres
+ *
+ * This function increases refcount of the host resource.
+ *
+ * @param[in]  res  handle to the host resource
+ */
+void nnpdrv_hostres_get(struct nnpdrv_host_resource *res);
+
+/**
+ * @brief Decreases refcount of the hostres and destroyes it when it reaches 0
+ *
+ * This function decreases refcount of the host resource and destroyes it
+ * when it reaches 0. Returns true if the host resource was destroyed.
+ *
+ * @param[in]  res  handle to the host resource
+ * @return true if destroy happened.
+ */
+bool nnpdrv_hostres_put(struct nnpdrv_host_resource *res);
+
+/**
+ * @brief Returns if the host resource is input resource
+ *
+ * This function returns true if the host resource can be read by device.
+ *
+ * @param[in]  res  handle to the host resource
+ * @return true if the reasource is readable.
+ */
+bool nnpdrv_hostres_is_input(struct nnpdrv_host_resource *res);
+
+/**
+ * @brief Returns if the host resource is output resource
+ *
+ * This function returns true if the host resource can be modified by device.
+ *
+ * @param[in]  res  handle to the host resource
+ * @return true if the reasource is writable.
+ */
+bool nnpdrv_hostres_is_output(struct nnpdrv_host_resource *res);
+
+/**
+ * @brief Returns size of the host resource
+ *
+ * This function returns size of the host resource or zero in case of failure.
+ *
+ * @param[in]  res  handle to the host resource
+ * @return true if the reasource is writable.
+ */
+size_t nnpdrv_hostres_get_size(struct nnpdrv_host_resource *res);
+
+bool nnpdrv_hostres_is_usermem(struct nnpdrv_host_resource *res);
+
+int nnpdrv_hostres_init_sysfs(struct kobject *kobj);
+void nnpdrv_hostres_fini_sysfs(struct kobject *kobj);
+
+#endif
diff --git a/drivers/misc/intel-nnpi/nnp_log.h b/drivers/misc/intel-nnpi/nnp_log.h
index b62e78137430..838d42f3c2a6 100644
--- a/drivers/misc/intel-nnpi/nnp_log.h
+++ b/drivers/misc/intel-nnpi/nnp_log.h
@@ -13,6 +13,7 @@
 #define GENERAL_LOG "NNPLOG_GENERAL"
 #define START_UP_LOG "NNPLOG_START_UP"
 #define GO_DOWN_LOG "NNPLOG_GO_DOWN"
+#define CREATE_COMMAND_LOG "NNPLOG_CREATE_COMMAND"
 
 #define nnp_log_debug(category, fmt, arg...)   pr_debug(KBUILD_MODNAME ", " category " , DEBUG, %s: " fmt, __func__, ##arg)
 #define nnp_log_info(category, fmt, arg...)    pr_info(KBUILD_MODNAME ", " category " , INFO, %s: " fmt, __func__, ##arg)
-- 
2.22.0

